{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.font_manager as fm\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from tensorflow.keras import models, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "#User Created functions\n",
    "\n",
    "from cleaning_functions import *\n",
    "from eda_functions import *\n",
    "from modeling_functions import *\n",
    "from setup_functions import *\n",
    "\n",
    "from random_lumberjacks.src.random_lumberjacks.model.model_classes import *\n",
    "from random_lumberjacks.src.random_lumberjacks.visualization.visualization_functions import *\n",
    "\n",
    "#Notebook arguments\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anacuda/work/nyc-mhtn-ds-021720/japanese_text_classifiers/setup_functions.py:22: MatplotlibDeprecationWarning: \n",
      "The createFontList function was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use FontManager.addfont instead.\n",
      "  font_list = fm.createFontList(font_files)\n"
     ]
    }
   ],
   "source": [
    "#Without this block the Japanese font's won't display properly in Matplotlib.Set to your font directory.\n",
    "extend_matplotlib_fonts(\"/usr/share/fonts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device specific gpu deterministic arguments\n",
    "from tensorflow import config as tfconfig\n",
    "physical_devices = tfconfig.list_physical_devices('GPU')\n",
    "tfconfig.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets random seeds to allow for reproducable results.\n",
    "from tensorflow import random as tfrandom\n",
    "SEED=127\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tfrandom.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing and splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kuzushiji Kanji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_kuzushiji_kanji_dir = 'data/kuzushiji49/kkanji2'\n",
    "split_kanji_dir = \"data/kuzushiji49/kanji_transfer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>count</th>\n",
       "      <th>char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U+5375</td>\n",
       "      <td>197</td>\n",
       "      <td>卵</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U+5BC6</td>\n",
       "      <td>30</td>\n",
       "      <td>密</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U+85A6</td>\n",
       "      <td>6</td>\n",
       "      <td>薦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U+8DE1</td>\n",
       "      <td>109</td>\n",
       "      <td>跡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U+7409</td>\n",
       "      <td>2</td>\n",
       "      <td>琉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>U+5B63</td>\n",
       "      <td>37</td>\n",
       "      <td>季</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828</th>\n",
       "      <td>U+75B3</td>\n",
       "      <td>2</td>\n",
       "      <td>疳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>U+9322</td>\n",
       "      <td>9</td>\n",
       "      <td>錢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>U+675C</td>\n",
       "      <td>12</td>\n",
       "      <td>杜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>U+5EFC</td>\n",
       "      <td>3</td>\n",
       "      <td>廼</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3832 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      folder  count char\n",
       "0     U+5375    197    卵\n",
       "1     U+5BC6     30    密\n",
       "2     U+85A6      6    薦\n",
       "3     U+8DE1    109    跡\n",
       "4     U+7409      2    琉\n",
       "...      ...    ...  ...\n",
       "3827  U+5B63     37    季\n",
       "3828  U+75B3      2    疳\n",
       "3829  U+9322      9    錢\n",
       "3830  U+675C     12    杜\n",
       "3831  U+5EFC      3    廼\n",
       "\n",
       "[3832 rows x 3 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_sizes = [[directory, len(os.listdir(f\"{orig_kuzushiji_kanji_dir}/{directory}\"))] for directory in os.listdir(orig_kuzushiji_kanji_dir)]\n",
    "directory_counts = pd.DataFrame(directory_sizes, columns=[\"folder\", \"count\"])\n",
    "directory_counts[\"char\"] = directory_counts[\"folder\"].map(lambda x: chr(int(x[2:], 16)))\n",
    "directory_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [[i, directory_counts[directory_counts[\"count\"] >= i][\"count\"].size, directory_counts[directory_counts[\"count\"] >= i][\"count\"].sum()] for i in np.arange(1, 1769)]\n",
    "sizes = pd.DataFrame(sizes, columns = [\"min\", \"classes\", \"total_chars\"]).set_index(\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9d99aebf90>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4AAAAJNCAYAAABgNRuJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdebyVVaH/8c/iHOZJ4DAjggzKDHIEwlkUtDLNKbQSC7PU0p/da2laekuvZt4oLfOalpqWmlZqXjUVDQccDgQKogKKAqJMijghHNbvj73RAx2mM609fN6v1349e69n8Hv447763vU86wkxRiRJkiRJha9R6gCSJEmSpIZhAZQkSZKkImEBlCRJkqQiYQGUJEmSpCJhAZQkSZKkImEBlCRJkqQiUZo6QF0rKyuLvXr1Sh1DkiRJkpKYMWPGyhhjx+r2FVwB7NWrFxUVFaljSJIkSVISIYTXtrbPW0AlSZIkqUhYACVJkiSpSFgAJUmSJKlIFNwzgJIkSZLSWr9+PUuWLOGjjz5KHaWgNWvWjB49etC4ceMdPscCKEmSJKlOLVmyhNatW9OrVy9CCKnjFKQYI6tWrWLJkiX07t17h8/zFlBJkiRJdeqjjz6iQ4cOlr96FEKgQ4cOOz3LagGUJEmSVOcsf/WvJv/GFkBJkiRJReGiiy7iiiuuSB0jKQugJEmSJBUJC6AkSZKkgnTTTTcxdOhQhg0bxle/+tXN9v32t79l7733ZtiwYRxzzDF88MEHAPz5z39m8ODBDBs2jP333x+AuXPnMmrUKIYPH87QoUOZP38+ADfffPMn49/85jeprKyksrKSk08+mcGDBzNkyBCmTJnSsH/0drgKqCRJkqSCM3fuXC655BKeeOIJysrKWL16NVdeeeUn+48++mi+8Y1vAHDBBRdw/fXX853vfIcf//jHPPDAA3Tv3p133nkHgGuuuYazzjqLL3/5y3z88cdUVlYyb948brvtNp544gkaN27M6aefzi233MKgQYNYunQpc+bMAfjkGrnCAihJkiSp3vzXPXN54Y136/SaA7u14cIjBm3zmKlTp3LsscdSVlYGQPv27TfbP2fOHC644ALeeecd3nvvPSZMmADAPvvsw8knn8zxxx/P0UcfDcBnPvMZLrnkEpYsWcLRRx9Nv379ePjhh5kxYwZ77703AB9++CGdOnXiiCOO4JVXXuE73/kOn/vc5xg/fnyd/u215S2gkiRJkgpOjHGbq2SefPLJ/OpXv+L555/nwgsv/OR1Ctdccw0XX3wxixcvZvjw4axatYoTTzyRu+++m+bNmzNhwgSmTp1KjJFJkyYxa9YsZs2axUsvvcRFF11Eu3btmD17NgceeCC//vWvOeWUUxrqT94hzgBKkiRJqjfbm6mrL+PGjeOLX/wiZ599Nh06dGD16tWb7V+7di1du3Zl/fr13HLLLXTv3h2AhQsXMnr0aEaPHs0999zD4sWLWbNmDbvvvjtnnnkmr7zyCs899xzjx4/nyCOP5Oyzz6ZTp06sXr2atWvX0rJlS5o0acIxxxxDnz59OPnkkxP89VtnAZQkSZJUcAYNGsT555/PAQccQElJCSNGjKBXr16f7P/JT37C6NGj2W233RgyZAhr164F4JxzzmH+/PnEGBk3bhzDhg3jsssu4+abb6Zx48Z06dKFH/3oR7Rv356LL76Y8ePHs3HjRho3bsyvf/1rmjdvzte+9jU2btwIwKWXXpriz9+qEGNMnaFOlZeXx4qKitQxJEmSpKI1b948BgwYkDpGUaju3zqEMCPGWF7d8T4DKEmSJElFwgIoSZIkSUXCAihJkiRJRcICKEmSJElFYrsFMITwuxDC8hDCnGr2/WcIIYYQyqqMnRdCWBBCeCmEMKHK+MgQwvPZfVeG7Es5QghNQwi3ZcefDiH0qnLOpBDC/OxnUm3/WEmSJEkqZjsyA3gDcNiWgyGEXYFDgderjA0EJgKDsudcHUIoye7+DXAq0C/72XTNycDbMca+wBTgp9lrtQcuBEYDo4ALQwjtdu7PkyRJkiRtst0CGGOcBqyuZtcU4HtA1fdIHAncGmNcF2N8FVgAjAohdAXaxBinx8x7J24Cjqpyzo3Z73cA47KzgxOAB2OMq2OMbwMPUk0RzQuvPwVXjoA3n0+dRJIkSVIRq9EzgCGELwBLY4yzt9jVHVhc5feS7Fj37Pctxzc7J8a4AVgDdNjGtfJP09aw+hVY8VLqJJIkSVJReOedd7j66qu3ecyiRYv44x//uN1rLVq0iMGDB+90hl69erFy5cqdPq8+7XQBDCG0AM4HflTd7mrG4jbGa3rOlplODSFUhBAqVqxYUd0habXvAwRYtSB1EkmSJKko1GUBTGHDhg31ct2azAD2AXoDs0MIi4AewMwQQhcys3S7Vjm2B/BGdrxHNeNUPSeEUAq0JXPL6dau9W9ijNfGGMtjjOUdO3aswZ9Uzxo3g116wsqXUyeRJEmSisK5557LwoULGT58OOeccw7nnHMOgwcPZsiQIdx2222fHPPYY48xfPhwpkyZwqJFi9hvv/3Ya6+92GuvvXjyySd36L9VWVnJf/7nfzJkyBCGDh3KVVdd9cm+q666ir322oshQ4bw4osvAvDMM88wduxYRowYwdixY3nppcydgjfccAPHHXccRxxxBOPHj2fZsmXsv//+DB8+nMGDB/PYY4/V+t+ldGdPiDE+D3Ta9DtbAstjjCtDCHcDfwwh/BzoRmaxl2dijJUhhLUhhDHA08BJwKZ/lbuBScB04FhgaowxhhAeAP67ysIv44HzavJH5oSy/rByfuoUkiRJUsO679y6XwujyxA4/LJtHnLZZZcxZ84cZs2axZ133sk111zD7NmzWblyJXvvvTf7778/l112GVdccQV///vfAfjggw948MEHadasGfPnz+eEE06goqJiu3GuvfZaXn31Vf71r39RWlrK6tWfLqFSVlbGzJkzufrqq7niiiu47rrr2HPPPZk2bRqlpaU89NBD/OAHP+DOO+8EYPr06Tz33HO0b9+e//mf/2HChAmcf/75VFZW8sEHH9TiHy1juwUwhPAn4ECgLISwBLgwxnh9dcfGGOeGEG4HXgA2AGfEGCuzu08js6Joc+C+7AfgeuAPIYQFZGb+JmavtTqE8BPg2exxP44xVrcYTX4o6wevPQEbN0IjX78oSZIkNZTHH3+cE044gZKSEjp37swBBxzAs88+S5s2bTY7bv369Xz7299m1qxZlJSU8PLLO3YH30MPPcS3vvUtSksz9ap9+/af7Dv66KMBGDlyJH/5y18AWLNmDZMmTWL+/PmEEFi/fv0nxx966KGfnL/33nvz9a9/nfXr13PUUUcxfPjwmv8jZG23AMYYT9jO/l5b/L4EuKSa4yqAf3tyMsb4EXDcVq79O+B328uYFzr0hfUfwNo3oG2P7R8vSZIkFYLtzNQ1hMyLCLZvypQpdO7cmdmzZ7Nx40aaNWu2w9fPvub83zRt2hSAkpKST57r++EPf8hBBx3EX//6VxYtWsSBBx74yfEtW7b85Pv+++/PtGnTuPfee/nqV7/KOeecw0knnbRDmbbGqaiGUtY/s/U5QEmSJKnetW7dmrVr1wKZInXbbbdRWVnJihUrmDZtGqNGjdrsGMjMzHXt2pVGjRrxhz/8gcrKyq1dfjPjx4/nmmuu+aTgVb0FtDpr1qyhe/fMCw5uuOGGrR732muv0alTJ77xjW8wefJkZs6cuUN5tsUC2FDK+mW2K10JVJIkSapvHTp0YJ999mHw4MFMnz6doUOHMmzYMA4++GAuv/xyunTpwtChQyktLWXYsGFMmTKF008/nRtvvJExY8bw8ssvbzYbty2nnHIKPXv2/OS/sb2VRb/3ve9x3nnnsc8++2yzZD766KMMHz6cESNGcOedd3LWWWft1L9BdcKOTofmi/Ly8rgjD2o2uBjhsp4w9EvwuStSp5EkSZLqzbx58xgwYEDqGEWhun/rEMKMGGN5dcc7A9hQQsg8B7jKlUAlSZIkpbHTr4FQLZT1h0W1f3eHJEmSpIb3wAMP8P3vf3+zsd69e/PXv/41UaKdZwFsSGV94blbYd170LRV6jSSJEmSdsKECROYMGFC6hi14i2gDWnTSqCrXAhGkiRJha3Q1hrJRTX5N7YANqQO2ZVALYCSJEkqYM2aNWPVqlWWwHoUY2TVqlU7/K7CTbwFtCG13x0IsNKFYCRJklS4evTowZIlS1ixYkXqKAWtWbNm9OjRY6fOsQA2pMbNoN1uvgxekiRJBa1x48b07t07dQxVw1tAG1qHfr4KQpIkSVISFsCGVtYfVi6AjRtTJ5EkSZJUZCyADa2sL2z4EN5dmjqJJEmSpCJjAWxom14F4XOAkiRJkhqYBbCh+SoISZIkSYlYABtaq07QtK0zgJIkSZIanAWwoYUAZf1gxUupk0iSJEkqMhbAFDoPgjefhxhTJ5EkSZJURCyAKXQZAh+9A+++kTqJJEmSpCJiAUyh8+DM9q05aXNIkiRJKioWwBQ6D8xs33w+bQ5JkiRJRcUCmEKztrBLT1j+QuokkiRJkoqIBTCVzoPhrbmpU0iSJEkqIhbAVDoNhJXzYcO61EkkSZIkFQkLYCqdB0Gs9H2AkiRJkhqMBTAVVwKVJEmS1MAsgKl06AONW8Cy2amTSJIkSSoSFsBUGpVA12Hwxr9SJ5EkSZJUJCyAKXUbAcueg8oNqZNIkiRJKgIWwJS6jYANH8KKF1MnkSRJklQELIApdRuR2XobqCRJkqQGYAFMqX0faNLaAihJkiSpQVgAU2rUCLoNtwBKkiRJahAWwNS6DoO35kLl+tRJJEmSJBU4C2BqXYdD5TpY8VLqJJIkSZIKnAUwtW7DM9tls9LmkCRJklTwLICpte8DTVrBstmpk0iSJEkqcBbA1Bo1gk4DYPm81EkkSZIkFTgLYC7oNDCzEEyMqZNIkiRJKmAWwFzQcU/4cDV8sDp1EkmSJEkFzAKYCzr0zWxXLUibQ5IkSVJBswDmgg59MlsLoCRJkqR6ZAHMBbvsBo1KLYCSJEmS6pUFMBeUlEK73hZASZIkSfXKApgrOvS1AEqSJEmqVxbAXNF5EKx4CT7+IHUSSZIkSQXKApgrepRDrIRls1MnkSRJklSgLIC5ont5Zru0Im0OSZIkSQXLApgrWnWEXXrCEgugJEmSpPphAcwl3cth6YzUKSRJkiQVKAtgLuk+EtYshrVvpU4iSZIkqQBZAHNJj03PAToLKEmSJKnuWQBzSddh0KjUhWAkSZIk1QsLYC5p3DzzPkBnACVJkiTVAwtgruk+EpbOhI2VqZNIkiRJKjAWwFzTa19Y966zgJIkSZLqnAUw1/Q5GAiwcGrqJJIkSZIKjAUw1zRvB50GwOJnUieRJEmSVGAsgLmox96wpAI2bkydRJIkSVIBsQDmol1Hwbo1sGp+6iSSJEmSCogFMBd1z74Q/o1/pc0hSZIkqaBYAHNRhz5Q0gSWv5A6iSRJkqQCst0CGEL4XQhheQhhTpWxn4UQXgwhPBdC+GsIYZcq+84LISwIIbwUQphQZXxkCOH57L4rQwghO940hHBbdvzpEEKvKudMCiHMz34m1dUfnfNKGkNZf1g+L3USSZIkSQVkR2YAbwAO22LsQWBwjHEo8DJwHkAIYSAwERiUPefqEEJJ9pzfAKcC/bKfTdecDLwdY+wLTAF+mr1We+BCYDQwCrgwhNBu5//EPNVpoAVQkiRJUp3abgGMMU4DVm8x9o8Y44bsz6eAHtnvRwK3xhjXxRhfBRYAo0IIXYE2McbpMcYI3AQcVeWcG7Pf7wDGZWcHJwAPxhhXxxjfJlM6tyyihavTAFizGD5akzqJJEmSpAJRF88Afh24L/u9O7C4yr4l2bHu2e9bjm92TrZUrgE6bONaxaHL0MzWhWAkSZIk1ZFaFcAQwvnABuCWTUPVHBa3MV7Tc7bMcWoIoSKEULFixYpth84Xu+4NoRG8Nj11EkmSJEkFosYFMLsoy+eBL2dv64TMLN2uVQ7rAbyRHe9Rzfhm54QQSoG2ZG453dq1/k2M8doYY3mMsbxjx441/ZNyS7O20GUIvPZE6iSSJEmSCkSNCmAI4TDg+8AXYowfVNl1NzAxu7JnbzKLvTwTY1wGrA0hjMk+33cScFeVczat8HksMDVbKB8AxocQ2mUXfxmfHSseu+0LS56FDetSJ5EkSZJUAHbkNRB/AqYDe4QQloQQJgO/AloDD4YQZoUQrgGIMc4FbgdeAO4HzogxVmYvdRpwHZmFYRby6XOD1wMdQggLgO8C52avtRr4CfBs9vPj7Fjx2G0sbPjI5wAlSZIk1Ynw6d2bhaG8vDxWVFSkjlE33l8FP9sdxv0I9vuP1GkkSZIk5YEQwowYY3l1++piFVDVl5YdoOMAeO3J1EkkSZIkFQALYK7rOTrzHGCBzdRKkiRJangWwFzXaWDmZfDvvZU6iSRJkqQ8ZwHMdR33zGzfmpM2hyRJkqS8ZwHMdd1HQuOW8MJd2z9WkiRJkrbBApjrmraCPge5EIwkSZKkWrMA5oMuQ2DVQvj4/dRJJEmSJOUxC2A+6DYCiPDa9NRJJEmSJOUxC2A+2P1AaNoWXvhb6iSSJEmS8pgFMB+UNoVe+/gcoCRJkqRasQDmi55jYPVCeG956iSSJEmS8pQFMF/0HJvZvv5U2hySJEmS8pYFMF90HQalzeB1F4KRJEmSVDMWwHxR2gS6l/scoCRJkqQaswDmk177wpvPwYdvp04iSZIkKQ9ZAPNJn4MhboRX/pk6iSRJkqQ8ZAHMJ91HQtM2sPDh1EkkSZIk5SELYD4pKYWen4HFz6ZOIkmSJCkPWQDzTedBsGo+bPg4dRJJkiRJecYCmG86D4KNG2DFvNRJJEmSJOUZC2C+6fmZzNaFYCRJkiTtJAtgvmnbHTruCQunpk4iSZIkKc9YAPNRn3GZF8Kv/zB1EkmSJEl5xAKYj/ocDJXr4LUnUieRJEmSlEcsgPlot7FQ2hxevDd1EkmSJEl5xAKYj5q0gD0Oh3n3QIyp00iSJEnKExbAfNVzDLy/At57K3USSZIkSXnCApivOg/KbN+ckzaHJEmSpLxhAcxXXYZCKIHFT6VOIkmSJClPWADzVbM20G04vPpY6iSSJEmS8oQFMJ/13h+WVsDH76dOIkmSJCkPWADz2e4HwsYNsPCR1EkkSZIk5QELYD7bbR9o3i7zOghJkiRJ2g4LYD4raZyZBXx1mu8DlCRJkrRdFsB812s/WPsGrH4ldRJJkiRJOc4CmO9675/ZvvJo0hiSJEmScp8FMN916Atl/WHmTd4GKkmSJGmbLID5LgQY/S1YNstZQEmSJEnbZAEsBMNOgLY94ZFLUieRJEmSlMMsgIWgSQsYdCQsew4q16dOI0mSJClHWQALRZdhULkOVr6cOokkSZKkHGUBLBRdhmS2y55Lm0OSJElSzrIAFoqyflDaHN60AEqSJEmqngWwUDQqge57waLHUieRJEmSlKMsgIWk7yHw5vPw3vLUSSRJkiTlIAtgIdltn8x28TNpc0iSJEnKSRbAQtJ1GJQ0hVenpU4iSZIkKQdZAAtJ42bQfwLM/QtUbkidRpIkSVKOsQAWmqHHw/srYNbNqZNIkiRJyjEWwELTbwJ0L4d7zoJ596ROI0mSJCmHWAALTWkTOPnv0KEfPHlV6jSSJEmScogFsBA1bg5Djs2sBvr+qtRpJEmSJOUIC2Ch6nMwEH0xvCRJkqRPWAALVbcR0LilBVCSJEnSJyyAhaqkMew2FhY85CshJEmSJAEWwMI29Evw9iJ47tbUSSRJkiTlAAtgIRtyLLTqDK88mjqJJEmSpBxgASxkIUDv/WHhVG8DlSRJkmQBLHgDvgAfrILFT6dOIkmSJCkxC2Ch6zkms102O20OSZIkSclZAAtdq06Z5wCXzUqdRJIkSVJi2y2AIYTfhRCWhxDmVBlrH0J4MIQwP7ttV2XfeSGEBSGEl0IIE6qMjwwhPJ/dd2UIIWTHm4YQbsuOPx1C6FXlnEnZ/8b8EMKkuvqji06fcfDivfDx+6mTSJIkSUpoR2YAbwAO22LsXODhGGM/4OHsb0IIA4GJwKDsOVeHEEqy5/wGOBXol/1suuZk4O0YY19gCvDT7LXaAxcCo4FRwIVVi6Z2wpBj4OP3YPEzqZNIkiRJSmi7BTDGOA1YvcXwkcCN2e83AkdVGb81xrguxvgqsAAYFULoCrSJMU6PMUbgpi3O2XStO4Bx2dnBCcCDMcbVMca3gQf59yKqHdFjbyC4EIwkSZJU5Gr6DGDnGOMygOy2U3a8O7C4ynFLsmPds9+3HN/snBjjBmAN0GEb19LOatYWOg20AEqSJElFrq4XgQnVjMVtjNf0nM3/oyGcGkKoCCFUrFixYoeCFp2eo2Hxs7CxMnUSSZIkSYnUtAC+lb2tk+x2eXZ8CbBrleN6AG9kx3tUM77ZOSGEUqAtmVtOt3atfxNjvDbGWB5jLO/YsWMN/6QCt+sY+HgtLH8hdRJJkiRJidS0AN4NbFqVcxJwV5XxidmVPXuTWezlmextomtDCGOyz/edtMU5m651LDA1+5zgA8D4EEK77OIv47NjqolN7wNc+EjaHJIkSZKSKd3eASGEPwEHAmUhhCVkVua8DLg9hDAZeB04DiDGODeEcDvwArABOCPGuOmew9PIrCjaHLgv+wG4HvhDCGEBmZm/idlrrQ4h/AR4Nnvcj2OMWy5Gox3VbrfMYjDTfgaDvgi77Lr9cyRJkiQVlJCZbCsc5eXlsaKiInWM3LR0Bvz2YJhwKXzm9NRpJEmSJNWDEMKMGGN5dfvqehEY5bLuI6H97pmXwkuSJEkqOhbAYjPya/Da4/DmnNRJJEmSJDUwC2CxGTYRQiN44a7tHytJkiSpoFgAi02rTrDbPvDC36DAnv+UJEmStG0WwGI09Euw8mV4+f7USSRJkiQ1IAtgMRr6Jei4J/zfObDh49RpJEmSJDUQC2AxKm0Ch/4E1iyGF+9JnUaSJElSA7EAFqu+46B1N5h9W+okkiRJkhqIBbBYNSqBocfBgofg/ZWp00iSJElqABbAYjZ0IsRKmHNn6iSSJEmSGoAFsJh1Hgidh8DsW1MnkSRJktQALIDFbtiX4I2ZsHJ+6iSSJEmS6pkFsNgNPhZCCcy8MXUSSZIkSfXMAljs2nSFPQ6HOX9JnUSSJElSPbMACnqUw7tL4aM1qZNIkiRJqkcWQEHHPTPb5fPS5pAkSZJUryyAgh57Q0kTeM6XwkuSJEmFzAIoaFkGe50EM26AFS+nTiNJkiSpnlgAlXHAudCoFB6fkjqJJEmSpHpiAVRGq44w5jSY/Ud46b7UaSRJkiTVAwugPnXAuZkFYf52GlRuSJ1GkiRJUh2zAOpTTVrAQT+AD9+GVx5JnUaSJElSHbMAanP9JkC7XvDXb8HrT6dOI0mSJKkOWQC1ucbN4Mt3QLM2cNOR8PZrqRNJkiRJqiMWQP27sn4w6R7YuAEqrk+dRpIkSVIdsQCqem17QM8xMP+h1EkkSZIk1RELoLau36GwfC6sWZo6iSRJkqQ6YAHU1vU9JLNd4CygJEmSVAgsgNq6TgOhdTdY8GDqJJIkSZLqgAVQWxcC9DsEXvknVK5PnUaSJElSLVkAtW39D4d178Lcv6VOIkmSJKmWLIDatv6HQYd+MPPG1EkkSZIk1ZIFUNvWqBEM+Dy8Ph0+fCd1GkmSJEm1YAHU9vU/PPNS+IVTUyeRJEmSVAsWQG1fj3Jo3h5evj91EkmSJEm1YAHU9jUqyTwL+NJ9sPat1GkkSZIk1ZAFUDtm7Ldh/Yfw8wGwckHqNJIkSZJqwAKoHdN5EEy8BWIlPHdr6jSSJEmSasACqB3XfwLsfiA8fwfEmDqNJEmSpJ1kAdTOGXYivP2qC8JIkiRJecgCqJ0z+Bho1tYCKEmSJOUhC6B2TkkpdBsBS2emTiJJkiRpJ1kAtfN2HQNvzYH3lqdOIkmSJGknWAC18wYeCXEjzLs7dRJJkiRJO8ECqJ3XaQCU7QFz/5Y6iSRJkqSdYAHUzgsBBh0Frz3hbaCSJElSHrEAqmb6TcjcBvr69NRJJEmSJO0gC6BqpvMgCCWwdEbqJJIkSZJ2kAVQNdO4Gex+ADx1DSx4OHUaSZIkSTvAAqiaO+Z6KOsHfz4ZPnwndRpJkiRJ22EBVM21aA9HXQ3r3oVnr0udRpIkSdJ2WABVO12HQd9D4anfwMcfpE4jSZIkaRssgKq9MafBByth0eOpk0iSJEnaBgugaq9HeWb71vNpc0iSJEnaJgugaq9ZW9ilJ7w5J3USSZIkSdtgAVTd6DIU3rIASpIkSbnMAqi60XkwrFrgQjCSJElSDrMAqm50GQJxIyyflzqJJEmSpK2wAKpudBmc2b4xM20OSZIkSVtlAVTd2GU3aNcbnvglxJg6jSRJkqRqWABVN0KAff8frFkMb/wrdRpJkiRJ1bAAqu7stm9me+fktDkkSZIkVatWBTCEcHYIYW4IYU4I4U8hhGYhhPYhhAdDCPOz23ZVjj8vhLAghPBSCGFClfGRIYTns/uuDCGE7HjTEMJt2fGnQwi9apNX9aysL/QbD28vgnVrU6eRJEmStIUaF8AQQnfgTKA8xjgYKAEmAucCD8cY+wEPZ38TQhiY3T8IOAy4OoRQkr3cb4BTgX7Zz2HZ8cnA2zHGvsAU4Kc1zasGMua0zGqgz16XOokkSZKkLdT2FtBSoHkIoRRoAbwBHAncmN1/I3BU9vuRwK0xxnUxxleBBcCoEEJXoE2McXqMMQI3bXHOpmvdAYzbNDuoHLX7QdD3kMxiML4TUJIkScopNS6AMcalwBXA68AyYE2M8R9A5xjjsuwxy4BO2VO6A4urXGJJdqx79vuW45udE2PcAKwBOtQ0sxpACLDvd+HDt+GmL8D6D1MnkiRJkpRVm1tA25GZoesNdANahhC+sq1TqhmL2xjf1jlbZjk1hFARQqhYsWLFtoOr/u02FvY6CZY8CzNvSp1GkiRJUlZtbgE9BHg1xrgixrge+AswFngre1sn2e3y7PFLgF2rnN+DzC2jS7Lftxzf7JzsbWzq4fIAACAASURBVKZtgdVbBokxXhtjLI8xlnfs2LEWf5LqRAjwhaugQz+473uwZmnqRJIkSZKoXQF8HRgTQmiRfS5vHDAPuBuYlD1mEnBX9vvdwMTsyp69ySz28kz2NtG1IYQx2euctMU5m651LDA1+5yg8sG+Z2e2T/wibQ5JkiRJQGYRlxqJMT4dQrgDmAlsAP4FXAu0Am4PIUwmUxKPyx4/N4RwO/BC9vgzYoyV2cudBtwANAfuy34Argf+EEJYQGbmb2JN8yqBEV+GpTPgmWuhXW8Y/U1oVLL98yRJkiTVi1BoE2rl5eWxoqIidQxtsmEd3HQkvD49szjMIRemTiRJkiQVtBDCjBhjeXX7avsaCGnbSpvCV/4CbXrA4z+HhVNTJ5IkSZKKlgVQ9a9JC/javZnvj1wKlevT5pEkSZKKlAVQDaNdLzjmeljyDDx5Zeo0kiRJUlGyAKrhDDkWeoyCx34OH7+fOo0kSZJUdCyAalh7HA4fvwd/Phk2bkydRpIkSSoqFkA1rH3+Hxx8Acz/B8x/IHUaSZIkqahYANWwGjWCsWdCaXNY8HDqNJIkSVJRsQCq4ZU2hf4TYOZN8M7i1GkkSZKkomEBVBqH/hdUroPZf0qdRJIkSSoaFkCl0a4X9NoPKn4HqxamTiNJkiQVBQug0hl7JqxdBlftBc9elzqNJEmSVPAsgEqn/3g45WHY/SC4/wfw5vOpE0mSJEkFzQKotHqUwzHXQdPWMPWS1GkkSZKkgmYBVHoty2D4CbDgQXh/Veo0kiRJUsGyACo3DP0SbNyQWRRGkiRJUr2wACo3dBkCA74Aj1wM036WOo0kSZJUkCyAyh1H/BJad4OpF8NrT6ZOI0mSJBUcC6ByR4v2MPGWzPeH/ittFkmSJKkAWQCVW7rvBYdcBIufgt+Og2evT51IkiRJKhgWQOWe0d/KLArz0Ttw73fhiStTJ5IkSZIKggVQuadxczj6Wjj1n9CyIzz4Q1jxcupUkiRJUt6zACp3NW0Fpz6a+f7MtSmTSJIkSQXBAqjc1rYHjDoVnr0Ols5MnUaSJEnKaxZA5b5xP4LSZvCnE+DNOanTSJIkSXnLAqjc17Q1DD8R3nsTrtkHrh4L8+5JnUqSJEnKOxZA5YfP/xy+PQNGfAWWz4XbvgLLnkudSpIkScorFkDlj7K+cOSv4ZvTMr9fn542jyRJkpRnLIDKP12GQuuu8PIDqZNIkiRJecUCqPwTAow5DRY+DC/emzqNJEmSlDcsgMpPY06HTgPh/74Hi56AGFMnkiRJknKeBVD5qaQxfH4KvLsUbvgszLwxdSJJkiQp51kAlb96joHvvgDtd4eK36dOI0mSJOU8C6DyW5tumVdDLJsF7y5LnUaSJEnKaRZA5b/+h2e2/7sfrHgpbRZJkiQph1kAlf86DYBhJ8D7K+DhH6dOI0mSJOUsC6DyXwjwxWug76Hw4t9h9q2pE0mSJEk5yQKownH0tdCmBzx1deokkiRJUk6yAKpwtGgPIyfBstnw5FWp00iSJEk5xwKowjLqG1DSBJ65Fh78Ebz+VOpEkiRJUs6wAKqwNG8Hn/0ZvL8yMwt43/dSJ5IkSZJyhgVQhWfkyXD+MjjoB5nbQR//RepEkiRJUk6wAKpwlU+GzoPh0UthzZLUaSRJkqTkLIAqXC3aw8Q/Qozw24Nh7t9SJ5IkSZKSsgCqsLXbDQ7/Kbz3Fjxwfuo0kiRJUlIWQBW+8q/BhEvh3SXwjwtSp5EkSZKSsQCqOIycBD3HZlYGnXdP6jSSJElSEhZAFYcmLeG430PjFnDbV2DmH1InkiRJkhqcBVDFo3UX+Paz0LJj5v2A7y5LnUiSJElqUBZAFZe2PeBr98OGj+CXQ+G16bBkBnz8QepkkiRJUr2zAKr4lPWFz14BlR/D7w+D6w6Gu85InUqSJEmqd6WpA0hJ7D0Zuu8F762AF+6CWTfD4ZdDq46pk0mSJEn1xhlAFa9uI6D/eBh5cub3oseSxpEkSZLqmwVQ6jYCWnWBZ69PnUSSJEmqVxZAqaQU9j0bXns8cztojKkTSZIkSfXCAihB5jbQVl3g9pPgb6fBsucsgpIkSSo4FkAJoHEz+PKfoXs5zP4T/O9+8C9fFi9JkqTCYgGUNuk6FE7+O3zlTug8BJ69LnUiSZIkqU5ZAKWqGjeHvofAnp+FZbPhn5d7K6gkSZIKhgVQqs6en8tsH7kEFk5Nm0WSJEmqIxZAqTpdh8EFK6BpW5h3d+o0kiRJUp2wAEpbU9oEeo6GGTfAQxelTiNJkiTVWq0KYAhhlxDCHSGEF0MI80IInwkhtA8hPBhCmJ/dtqty/HkhhAUhhJdCCBOqjI8MITyf3XdlCCFkx5uGEG7Ljj8dQuhVm7zSTht7JrTuCo//AlYtTJ1GkiRJqpXazgD+Erg/xrgnMAyYB5wLPBxj7Ac8nP1NCGEgMBEYBBwGXB1CKMle5zfAqUC/7Oew7Phk4O0YY19gCvDTWuaVdk7v/eDUR6GkCTx5Veo0kiRJUq3UuACGENoA+wPXA8QYP44xvgMcCdyYPexG4Kjs9yOBW2OM62KMrwILgFEhhK5Amxjj9BhjBG7a4pxN17oDGLdpdlBqMK27wPATYNYf4eP3U6eRJEmSaqw2M4C7AyuA34cQ/hVCuC6E0BLoHGNcBpDddsoe3x1YXOX8Jdmx7tnvW45vdk6McQOwBuhQi8xSzfQcC5Xr4N1lqZNIkiRJNVabAlgK7AX8JsY4Anif7O2eW1HdzF3cxvi2ztn8wiGcGkKoCCFUrFixYtuppZpolf3/Y7z3VtockiRJUi3UpgAuAZbEGJ/O/r6DTCF8K3tbJ9nt8irH71rl/B7AG9nxHtWMb3ZOCKEUaAus3jJIjPHaGGN5jLG8Y8eOtfiTpK1o1TmztQBKkiQpj9W4AMYY3wQWhxD2yA6NA14A7gYmZccmAXdlv98NTMyu7NmbzGIvz2RvE10bQhiTfb7vpC3O2XStY4Gp2ecEpYa1qQDO/QtU/A4qN6TNI0mSJNVAaS3P/w5wSwihCfAK8DUypfL2EMJk4HXgOIAY49wQwu1kSuIG4IwYY2X2OqcBNwDNgfuyH8gsMPOHEMICMjN/E2uZV6qZ5u2g7a4w757M57Xp0H8CDDk2dTJJkiRph4VCm1ArLy+PFRUVqWOoEFWuh/Ufwq0nwqLHMmNf+BXs9dW0uSRJkqQqQggzYozl1e2r7XsApeJR0hiatYGT/w7/8XJm7O5vw8KpaXNJkiRJO8gCKNVE685w+lOZ7//8WdoskiRJ0g6q7TOAUvHqNADGnA7PXg93fuPT8bJ+cMD30uWSJEmStsICKNXGoC/CgodgybOZ3xs+gudvh7fmQmlTaFQK+/0HdOiTNqckSZKEBVCqnV1Hwbef/fT3R+/CzUfDslmZ32uWwpvPQbe9Pj1mj8/CHoc1bE5JkiQJC6BUt5q1gVMe+vT3Yz+Hp/8XXn4g83vdWnjxXhg5CUZ+DXbZNU1OSZIkFSVfAyE1pFcfgz+dAB+vhd77w5DjYfiXoZHrMUmSJKlu+BoIKVf03g9+sARGnwavTsu8RmLa5alTSZIkqUhYAKUUDrsUvr8ImraFRy+F1a+mTiRJkqQiYAGUUggBmreDr9+f+f3C39LmkSRJUlGwAEopdR4I7XrBG7NSJ5EkSVIRsABKqXUdBktnQIEtyCRJkqTcYwGUUut/OKxZDJfvDu++kTqNJEmSCpjvAZRSG3w0LH4aZvwefn945tnApq3h2BugZYfU6SRJklRAnAGUUittCkf8Ag78AZT1hxYdMq+IuPZAWPZc6nSSJEkqIM4ASrniwO9/+v1vp8OsW+CW4+Dbz0CztulySZIkqWA4AyjloqOuzswIvvcm3HWGC8RIkiSpTjgDKOWqA78PC6fCvHtg9q3QY+9P95WUwi67Zd4nKEmSJO0gC6CUy068FX7WF/72rX/fd9Q1MPyEhs8kSZKkvBVigd1aVl5eHisqKlLHkOrOm3Ng+bzNxx65GNYshcYtMrOBx1wPfQ5Kk0+SJEk5JYQwI8ZYXt0+ZwClXNdlcOZTVesu8OK9me/P3QYVFkBJkiRtnwVQyke998t8AOJGmHEDPPFLIPtMYIsOMPxEnxGUJEnSZiyAUr4b8WWo+B08+KPNx0uaQOeBme8tO0KrTg2fTZIkSTnFAijlu67D4AdLoXJ95vf6D+GXw+Avp3x6TNM2cM5CKG2SJqMkSZJyggVQKgSlTTMfgKat4Ov3w9uvZn4vfgam/wpWL4ROA9JllCRJUnK+CF4qRF2HwsAjM5+hx2fGHv4JvPLPtLkkSZKUlAVQKnRle0DHATD/AXjootRpJEmSlJAFUCp0jZvBGU9B+WRY+TIU2Ls/JUmStOMsgFKx6NgfPn4PrugPL92fOo0kSZISsABKxWLgUTDmdCDCjN+nTiNJkqQELIBSsWhZBoddCkOOh5fvz6wOKkmSpKJiAZSKzaCjMtvrD4WV89NmkSRJUoOyAErFZtdRcMSVme9vPpc2iyRJkhqUBVAqRpveDfj4lLQ5JEmS1KAsgFIxatwcOvSFN5+HFS+lTiNJkqQGYgGUitWkezLbefekzSFJkqQGYwGUilWbbtB9JLx4b+okkiRJaiAWQKmY7fl5eGMmrFmaOokkSZIagAVQKmYDjshsnQWUJEkqChZAqZiV9YOy/vCizwFKkiQVAwugVOz2/DwsegJuOR4euTR1GkmSJNWj0tQBJCU24ivw2pOwYh7M/0dmVrC0GZQ0gT4HQUnj1AklSZJURyyAUrHr0AcmP5B5H+CvR8Odkz/d9/kpUP71dNkkSZJUp0KMMXWGOlVeXh4rKipSx5Dy09uL4KN3M99vPwneewuatc383vNz8Ln/SRZNkiRJOyaEMCPGWF7dPmcAJX2qXa9Pvx9++aeLw7w1F2bcCLvsBh33gP4TksSTJElS7VgAJVWv//jMB2DpDLjuUHjwh9CoFM5ZCM13SZtPkiRJO81VQCVtX/eR8IM34KS7YOMGuLw3/OOHqVNJkiRpJ1kAJe2Yxs2g9wFw2E+h6zCY9UfYuDF1KkmSJO0EC6CkHRcCjPkWjDkDPlgJi5+GDeugwBaTkiRJKlQWQEk7r+84CI3g94fBxZ3g3v9InUiSJEk7wEVgJO28Fu3huBth1Xx4+R/w/B3QogPsfQq07pw6nSRJkrbCAiipZgZ+IbPtthfceiJMuzyzQMwhF6bNJUmSpK3yFlBJtdPnIDh/GfQcC4//HJbNTp1IkiRJW2EBlFQ3Dr4gs33ov2DOnWmzSJIkqVoWQEl1o9c+sOfnYeHDcMfXYe2bqRNJkiRpCz4DKKnufOlmWDYLrj0QfjkcGpVC+17wjUehxP9zI0mSlJr/i0xS3QkBug6HQ3+SmQF8dwm8cBc89j/QptvmxzYqgT0+C813SZNVkiSpCFkAJdWtEGCfMzPfP3wbFjwMj/539cfuezYcclFDJZMkSSp6FkBJ9ad5O/juC/DRu/++787JUPF7eG06jP4mDD664fNJkiQVGQugpPrVrG3ms6V9z4anr4E358ATv4BuI6o/v0UHaNamfjNKkiQVCQugpDT2ODzzeeRS+OdlcOXw6o9r3RXOnpt5ZlCSJEm1UusCGEIoASqApTHGz4cQ2gO3Ab2ARcDxMca3s8eeB0wGKoEzY4wPZMdHAjcAzYH/A86KMcYQQlPgJmAksAr4UoxxUW0zS8ohnzkDOvSFjRv+fd+yWZlZwkcugU4DYcixDZ9PkiSpgNTFDOBZwDxg0z1a5wIPxxgvCyGcm/39/RDCQGAiMAjoBjwUQugfY6wEfgOcCjxFpgAeBtxHpiy+HWPsG0KYCPwU+FIdZJaUK5q1gaHHVb+v7ziY+YfMKqIAnQdBpwENl02SJKnA1OpF8CGEHsDngOuqDB8J3Jj9fiNwVJXxW2OM62KMrwILgFEhhK5Amxjj9BhjJDPjd1Q117oDGBdCCLXJLCmPtOoE338VTnsy8/uOyfCnE+H9lWlzSZIk5alaFUDgF8D3gI1VxjrHGJcBZLedsuPdgcVVjluSHeue/b7l+GbnxBg3AGuADrXMLCmflDbN3P6510nQuBm8dC+8+HeIMfORJEnSDqtxAQwhfB5YHmOcsaOnVDMWtzG+rXO2zHJqCKEihFCxYsWKHYwjKW+EAF+4Ck55GFp1hnvOgv/aJfO57/up00mSJOWN2jwDuA/whRDCZ4FmQJsQws3AWyGErjHGZdnbO5dnj18C7Frl/B7AG9nxHtWMVz1nSQihFGgLrN4ySIzxWuBagPLycqcEpEIVAnzxf2Hx05nfL98Pc/4CA46A9rtDm25p80mSJOW4Gs8AxhjPizH2iDH2IrO4y9QY41eAu4FJ2cMmAXdlv98NTAwhNA0h9Ab6Ac9kbxNdG0IYk32+76Qtztl0rWOz/w0LnlTM+hwEB56b+ex9Cry/HG74HNx4ROpkkiRJOa8+3gN4GXB7CGEy8DpwHECMcW4I4XbgBWADcEZ2BVCA0/j0NRD3ZT8A1wN/CCEsIDPzN7Ee8krKV0MnZl4h8cLd8NSv4ZH/zqwSOuiLqZNJkiTlpFBoE2rl5eWxoqIidQxJDWnlArhmH9jwUeb3OQuhZVnaTJIkSYmEEGbEGMur21cfM4CS1LDK+sJ5S2HxU5nbQW/7KrRo/+n+gUdt/V2DkiRJRcQCKKkwlJRCj1HQbzy8+wasezcz/s7r8O5SC6AkSRIWQEmFpLQJfPnPm4/dfSa89H9p8kiSJOWY2r4IXpJyW9td4f0VsP7D1EkkSZKSswBKKmy79Mxsp16cNockSVIOsABKKmx7fg5KmsLzd8DGyu0fL0mSVMAsgJIKW9NWsN934b034en/TZ1GkiQpKQugpMI39jtAgMeugNWvpk4jSZKUjAVQUuFr0hJGfQM+WAV3Tk6dRpIkKRkLoKTiMOG/YdDRsHQGLJ2ZOo0kSVISFkBJxaGkMYw5LfN92s/SZpEkSUrEF8FLKh67joKhE+HFe+Gv2TLYtjscdD6EkDabJElSA7AASiouQ4+HxU/Bosdhw4eZl8QPOQ467pE6mSRJUr2zAEoqLn3HwVmzM99XLoBfjYQ7vg4tOmTGug6D8T9Jl0+SJKke+QygpOLVoQ8MOxGatIIN6+Cd1+HJK+GD1amTSZIk1QtnACUVrxDgi7/59Per0+DGI+D6Q2HsmTByUrpskiRJ9cAZQEnaZNfRMOKr8PH7UHF96jSSJEl1zgIoSZuUNoUjfwXDT4Rls2HuX1MnkiRJqlMWQEna0l4nZbZz7kybQ5IkqY75DKAkbaldr8yrIebcCZf13LFzyr8Oh1xUj6EkSZJqzwIoSdXZ97vQogyI2z/2lX/C83daACVJUs6zAEpSdToPhMMv27Fjn7wK/nEB/Kwf7H4gHPPb+kwmSZJUYz4DKEm1NeR4GPVN2KVn5rbRjz9InUiSJKlazgBKUm217gyfvRxe/D+49QS4ejSUNMnsGzoRDjgnbT5JkqQsZwAlqa7sfgCMPBl67A1dh0HcCDNuSJ1KkiTpE84ASlJdadISjvjlp7+f/BX843y4fjwQtn5e01Zw9G+hRft6jyhJkoqbM4CSVF8GHPH/27v3KLvq+u7j7+9cMpnc7yTkQgKkctEsLhFRRHnAC7Y8on1AQKpUadEWXXhbClptl30eF9ZK1adiF1UELVerKK2Wi0AREZCA8EAgQEi4hISEkIRcSTIzv+ePvYecuWYmM+fsOee8X2vtdfb+7d/e53vmlz3wmX05sPDd0DQ6+5L53iYSLP81rLyz6GolSVId8AygJJXL5APg7Ov779O2E742G+7+Njx9+572GYfDsR8vb32SJKnuGAAlqUhNLbDoA1n42/Ji1rZrOzz4YzjybGgZX2x9kiSpphgAJalo77u06/ITN8E1Z8BDV8ObPlZMTZIkqSZ5D6AkjTSzj85e7/nnYuuQJEk1xwAoSSPNuOnw1s/Apufg2Xtg9R+yaeMzRVcmSZKqnJeAStJItOBt8NtL4Icn72mLBvjUozBxdnF1SZKkqmYAlKSR6MAT4MM3wu7t2fLGZ+GmL8DqBw2AkiRpnxkAJWkkioAD375nefcOuPmL8JOPQGPz3rdvaoEP/Rz2P6J8NUqSpKpjAJSkatDcCqd+F9Yt3Xvfjg6497uw4r8NgJIkqQsDoCRViyPOGnjfx34Bf/hxdubwhAuzM4qSJKnu+RRQSapFR5wFOzbCnRfDK88XXY0kSRohDICSVItO/Bs485ps/sVHi61FkiSNGAZASapV+x2Wva41AEqSpIwBUJJqVct4mLwAXnyk6EokSdII4UNgJKmWzXw9LPslfH3+nrY3nA5//I3CSpIkScUxAEpSLTv+szB+fyBly8/+DpbeYACUJKlOGQAlqZbtf2Q2dbrnUrj5IrjpImgY4n8CmlvhLZ/MLjWVJElVwQAoSfXkwLdD62RY8sMh7ihB26swdSEsOn1YSpMkSeVnAJSkerLf4fCFZ4a+n7Zd8LVZsO6xoe9LkiRVjE8BlSQNXtMomHIQ/PYS2PpS0dVIkqQBMgBKkvbNER/MXlc/WGwdkiRpwAyAkqR9c/Q52esDV8J9l8HuHcXWI0mS9sp7ACVJ+6Z1Msx8Azzxy2waMwXecFrRVUmSpH54BlCStO/OuxO+8Gz2lRI+EEaSpBHPM4CSpH3X0Aitk7IHwtxzKTx0dZnfMOCkL++5/1CSJA2KAVCSNHQnfQWeurn877Psl9lkAJQkaZ8YACVJQ3foKdlUbts3wLrHs6+eGDe9/O8nSVKN8R5ASVL1mHEobHga/vFgePaeoquRJKnqGAAlSdXj2L+GU/4pm/f7ByVJGjQvAZUkVY8xU2DxR+G2r8LTd8CE2QPbburBMPP15a1NkqQqYACUJFWfWUfA8luzaSBaJ8PnV0JEeeuSJGmEMwBKkqrPGf8Gm54bWN9Hfwp3/SNsXQfj9ytvXZIkjXAGQElS9WkZB/sdNrC+W1bDXcDd34KJc4b+3k2j4YizoXn00PclSVKFGQAlSbVt5iJoHgv3Xjp8+xwzBQ5///DtT5KkCjEASpJq27gZ8PkV0L5z6Pva/Sp884/g5eVD35ckSQXY56+BiIi5EXFHRDweEUsj4oK8fUpE3BoRT+Wvk0u2uSgilkfEExHx7pL2oyPikXzddyKyu/QjoiUirsvb74uI+fv+USVJdat5NIyeOPRp/H4wfhYsvx0e/BHs2Fj0J5MkaVCG8j2AbcBnU0qHAscC50fEYcCFwG0ppYXAbfky+bozgcOBk4FLI6Ix39f3gPOAhfl0ct5+LrAxpXQw8E/A14dQryRJQ7f/UfDc7+DGT8KSy4uuRpKkQdnnAJhSWpNSejCf3wI8DswGTgWuzLtdCbwvnz8VuDaltDOltBJYDhwTEbOACSmle1JKCfhRt2069/XvwEmdZwclSSrEB34En1kG42bCei8FlSRVl2G5BzC/NPNI4D5gv5TSGshCYkTMyLvNBu4t2WxV3rY7n+/e3rnN8/m+2iLiFWAqsH446pYkadAam2DCrOzL5Vc/CI/+bGDbRQMc9D+yS0klSSrIkANgRIwDfgp8KqW0uZ8TdL2tSP2097dN9xrOI7uElHnz5u2tZEmShm7mG+C+78G/f2Tg2xz/OTjpy+WrSZKkvRhSAIyIZrLwd1VKqfNPoGsjYlZ+9m8WsC5vXwXMLdl8DrA6b5/TS3vpNqsiogmYCGzoXkdK6TLgMoDFixf3CIiSJA27d/09HP3nA+9/zRnw8lNlK0eSpIHY5wCY34v3A+DxlNIlJatuBM4BLs5ff1HSfnVEXALsT/awl9+nlNojYktEHEt2CemHgf/bbV/3AKcBt+f3CUqSVKzGZphxyMD7Tz0YXnoS1i7tv9/YGTBu+tBqkySpD0M5A3gc8CHgkYh4KG/7Ilnwuz4izgWeA04HSCktjYjrgcfIniB6fkqpPd/ur4ArgFbgv/IJsoD544hYTnbm78wh1CtJUnGmvQ6W/xq+95b++42elH1vYUNj//0kSdoHUWsn1BYvXpyWLFlSdBmSJHW1YyOsvItebmXfY+VdcP+/wqeXwsQ5ffeTJKkfEfFASmlxb+uG5SmgkiRpL1onw2Hv7b9Py4QsAG58xgAoSSoLA6AkSSPFlAXZ67+dBo2jiqtjzBQ4744stEqSaooBUJKkkWLSAfDOr8LmNcXVsPkFePxGWLcMDnhzcXVIksrCAChJ0kgRAcddUGwNLz2ZBcBXngcMgJJUaxqKLkCSJI0gk/Kv7F16AzxwBbTtLLQcSdLw8gygJEnao7kVZhwGT/wqm8ZMhUP/Z9FVSZKGiWcAJUlSVx/7DVzwcDa/8ZlCS5EkDS8DoCRJ6qqxOXsgzajxsOn5oquRJA0jLwGVJEk9RcCkefDglfDYz3vv8/bPwxv/orJ1SZKGxAAoSZJ6d+KX4Klbel+37Jfw5C0GQEmqMgZASZLUu0P+JJt6s3UdbHy2svVIkobMAChJkgZv4lxY+Rt4/D96rpt7LIybXvmaJEl7ZQCUJEmDN/11sGsrXPdnPdctOgP+9LLK1yRJ2isDoCRJGryjPwIHvAU62rq2/+enYcOKYmqSJO2VAVCSJA1eQwPMOLRn+7TXwfJfV74eSdKAGAAlSdLwmTgHtr4I157dd58DT4Bj/rJSFUmSShgAJUnS8Fn4TnjyJtj4TO/rN78Aqx8yAEpSQQyAkiRp+MxZDB+7s+/1t/9vuOub0N4Gjf5viCRVWkPRBUiSpDoycQ6kjuwyUUlSxfmnN0mSVDkT52av31oEEV3XLXwXnHVN5WuSpDpiAJQkSZUz/61w0ldg17au7St/AyvvKqYmSaojBkBJklQ5TS1w/Gd7tt/9HVh1P7z6CoyeWPm6JKlOGAAlSVLxJs7JXp+8GSYdMPz7jwaYtSgLoJJUxwyAkiSpeFMPyl5/VsavKU1nqwAAD3FJREFUh3jb5+HEL5Vv/5JUBQyAkiSpeDMXwbm/hp2by7P/Gz8JG54uz74lqYoYACVJUvEiYO4by7f/yQtg8+ry7V+SqoQBUJIk1b4J+2dPGn3q1mLrGDMFZh9dbA2S6poBUJIk1b6pB8Mj18NVpxVdCXx66Z6H3khShRkAJUlS7Xvrp2HhOyCl4mpY/Qf41edg03MGQEmFMQBKkqTa1zSq+EsvR43NXresKbYOSXXNAChJklQJ42dmrxtWwo5N5X2vUWOhsbm87yGpKhkAJUmSKmH0JBg1Dm7/+2wqp8kL4IKHyvsekqqSAVCSJKkSIuCMH8O6ZeV9n2fvhmX/CTu3QMv48r6XpKpjAJQkSaqUg07MpnIaMyULgFvWGgAl9dBQdAGSJEkaRuP2y163ri22DkkjkmcAJUmSaknnw2bu+JpfNzEcjvggHPj2oquQho0BUJIkqZZMXgBz3wSvPJ9N2ndb1sCurQZA1RQDoCRJUi1pHg3n3lJ0FbXhilNg2/qiq5CGlfcASpIkSb0ZOx22GwBVWwyAkiRJUm/GTodtLxVdhTSsvARUkiRJ6s3YafDqK3Dr32bf41grJs2DxR8tugoVxAAoSZIk9Wb2UdA8Fu69tOhKhk9HO6R2OPz90Dq56GpUAAOgJEmS1JuD3wFfWl10FcPr4evghvNg+wYDYJ3yHkBJkiSpXoyZmr1uf7nYOlQYA6AkSZJUL8bkZ/0MgHXLAChJkiTVi9fOAG4otg4VxnsAJUmSpHrRGQB/cT7c+MliaxmI4z8DJ/5N0VXUFAOgJEmSVC9axsOp34UNK4uuZO8evhae/33RVdQcA6AkSZJUT478s6IrGJi1j8LmGnsK6wjgPYCSJEmSRp7WybBjY9FV1BwDYAXsbu9g+bqtvLq7vehSJEmSpOrQOsWH1ZSBl4BWwG+Xr+cjP7yfn3z8zbxx/pSiy5EkSZJGvjGTYfc22PQ8NI4qupqemlqgdVLRVQyaAbACDpo2DoAVL201AEqSJEkDMXZG9vqt1xdbR58C/uI2mHN00YUMigGwAmZPbmVUYwMr1m8ruhRJkiSpOrz+f0E0QPuuoivpafvLcMf/gQ0rDIDqqbEhWDBtLI+t3lx0KZIkSVJ1aBkHR32o6Cp6t219FgCr8CE1PgSmQt66cBr3rdzA1p1tRZciSZIkaShG5/f+GQDVlz9ZNItdbR387MFVRZciSZIkaSgam2DUeAOg+nbk3EkcM38K37zlSZ54cUvR5UiSJEkaitbJ8OqmoqsYNANghUQE3zh9EaObGzj7+/dy74qXiy5JkiRJ0r5qnQQPXwMvP110JYNiAKygA6aO5eq/PJaxLU2cedm9/PVVD7DkmQ2klIouTZIkSdJgHP9ZOOJsGDW26EoGJWotfCxevDgtWbKk6DL6tX1XG/9y5wp+cNcKtu1qZ/r4Fo47aCrHHTyN4w6exv6TWosuUZIkSVKViogHUkqLe11XDQEwIk4Gvg00At9PKV3cV99qCICdtu5s49bHXuSOZS/xu6fXs35r9h0nsye1Mm/KGGZPbmXO5FZmT2rN5ieNYdak0TQ3euJWkiRJUu+qOgBGRCPwJPBOYBVwP3BWSumx3vpXUwAslVLiibVb+O1T63l41Su8sHE7L2zawdrNO7v0i4CZE0Yze1Irk8Y0M7alibEtTYxraWLsqCbGtjRm851tLT3bWpoaiIiCPqkkSZKkcuovAFbDF8EfAyxPKa0AiIhrgVOBXgNgtYoIDpk5gUNmTujSvrOtnTWbXuWFTTt4YeMOVuWvL2zazgubXmXbzja27Wxj6842drZ1DOi9GhuCMc2NNDc10NQQNDc20NwYNDVmy6Py9qbGBkY1NtDUWNKnoaGkf2d7z/107Z+ta2wIGhuChsg+b2MEDQ3QEEFDZOsi2DNPFngjsvmG2LOekuXOtsh/jqXLnX0AGhq6tkVAsKeeznUEr7U15H1669/5KkmSJFWLagiAs4HnS5ZXAW8qqJaKa2lqZP60scyftvebS9vaO9i2s52tu/aEwj0Bsb1L2/Zd7bR1dNDWntjdntjd3kFbRwe72xNt7R2vte3Y3c7uV0vb8/mOPX3aSpbrWZ4d8/lsLkrWZct7OvW2LnqsK9lPX+v2+r5RMt91XfS5rmewfa1P6WfpZds9/bu2RJ8L/W/bc133baPPdd0Nar8ly9Gtd88a+l7Z3/sM6me2l5r6W+zv59JjPwPcbqjKuu9+PtOQ912lf/Mp5x+ryv0jKe+/lTLuu0p/5uX9N16dx2a1/ryr9XdhNf5MvnzKYcycOLos+y6HagiAvY1Ul6QREecB5wHMmzevEjWNSE2NDUwc08DEMc2FvH9KibaORFt7Yld7B23tHbR1pNdCY3tHBx0J2jsSHSnR0QEdKdGeEikl2vPljo7Otmyg02vz2TaJrF9K+TogpbytpH9H6StAyXz37Ts6uu4HSrfP3vu1/iVtHem1D0/JbPaat+xZ7rmOLut69u9c7r6v0p95f+/bfR2l6/qpk27b9VZv5/t336bXOrvV3Ne67g2p29qen38QfftZ172Krvvt/p5919/fe/a2bV/vmW3bT02D6dtL/64F9r6y+z6HUznvPCjnn6HKectEeesu477Lt+v8DfyZ99h3tR6bVfrvcKTfKtUX/w32su/y7ZpdA7wKb6SohgC4CphbsjwHWF3aIaV0GXAZZPcAVq40lYqI/HJPaKWx6HIkSZIkdVMNj5O8H1gYEQsiYhRwJnBjwTVJkiRJUtUZ8WcAU0ptEfEJ4Gayr4G4PKW0tOCyJEmSJKnqjPgACJBS+hXwq6LrkCRJkqRqVg2XgEqSJEmShoEBUJIkSZLqhAFQkiRJkuqEAVCSJEmS6oQBUJIkSZLqhAFQkiRJkuqEAVCSJEmS6oQBUJIkSZLqhAFQkiRJkuqEAVCSJEmS6oQBUJIkSZLqhAFQkiRJkuqEAVCSJEmS6oQBUJIkSZLqhAFQkiRJkuqEAVCSJEmS6oQBUJIkSZLqhAFQkiRJkuqEAVCSJEmS6oQBUJIkSZLqRKSUiq5hWEXES8CzRdfRi2nA+qKL0IA5XtXDsaoujld1cbyqh2NVXRyv6lKN43VASml6bytqLgCOVBGxJKW0uOg6NDCOV/VwrKqL41VdHK/q4VhVF8erutTaeHkJqCRJkiTVCQOgJEmSJNUJA2DlXFZ0ARoUx6t6OFbVxfGqLo5X9XCsqovjVV1qary8B1CSJEmS6oRnACVJkiSpThgAKyAiTo6IJyJieURcWHQ99S4i5kbEHRHxeEQsjYgL8va/i4gXIuKhfPrjkm0uysfviYh4d3HV16eIeCYiHsnHZUneNiUibo2Ip/LXySX9Ha8CRMTrSo6fhyJic0R8ymNr5IiIyyNiXUQ8WtI26GMpIo7Oj8nlEfGdiIhKf5Z60Md4fSMilkXE/4uIGyJiUt4+PyJ2lBxn/1KyjeNVZn2M1aB/9zlWldHHeF1XMlbPRMRDeXvtHVspJacyTkAj8DRwIDAKeBg4rOi66nkCZgFH5fPjgSeBw4C/Az7XS//D8nFrARbk49lY9Oeopwl4BpjWre0fgAvz+QuBrzteI2fKf/e9CBzgsTVyJuBtwFHAoyVtgz6WgN8DbwYC+C/gPUV/tlqc+hivdwFN+fzXS8Zrfmm/bvtxvIoZq0H/7nOsihuvbuu/CXwln6+5Y8szgOV3DLA8pbQipbQLuBY4teCa6lpKaU1K6cF8fgvwODC7n01OBa5NKe1MKa0ElpONq4p1KnBlPn8l8L6SdsereCcBT6eUnu2nj2NVYSml3wAbujUP6liKiFnAhJTSPSn7P6AflWyjYdTbeKWUbkkpteWL9wJz+tuH41UZfRxbffHYKlh/45WfxfsAcE1/+6jm8TIAlt9s4PmS5VX0HzZUQRExHzgSuC9v+kR+Wc3lJZdBOYbFS8AtEfFARJyXt+2XUloDWagHZuTtjtfIcCZd/+PpsTVyDfZYmp3Pd29X5X2U7KxDpwUR8YeIuDMijs/bHK9iDeZ3n2M1MhwPrE0pPVXSVlPHlgGw/Hq7FthHr44AETEO+CnwqZTSZuB7wEHAEcAastP/4BiOBMellI4C3gOcHxFv66ev41WwiBgFvBf4Sd7ksVWd+hofx20EiIgvAW3AVXnTGmBeSulI4DPA1RExAcerSIP93edYjQxn0fUPmDV3bBkAy28VMLdkeQ6wuqBalIuIZrLwd1VK6WcAKaW1KaX2lFIH8K/suRTNMSxYSml1/roOuIFsbNbml190XoaxLu/ueBXvPcCDKaW14LFVBQZ7LK2i62WHjluFRcQ5wCnA2fmlZ+SXE76czz9Adl/ZH+F4FWYffvc5VgWLiCbgT4HrOttq8dgyAJbf/cDCiFiQ/1X8TODGgmuqa/m13T8AHk8pXVLSPquk2/uBzidD3QicGREtEbEAWEh2068qICLGRsT4znmyByA8SjYu5+TdzgF+kc87XsXr8tdTj60Rb1DHUn6Z6JaIODb/ffrhkm1UZhFxMvAF4L0ppe0l7dMjojGfP5BsvFY4XsUZ7O8+x2pEeAewLKX02qWdtXhsNRVdQK1LKbVFxCeAm8meind5SmlpwWXVu+OADwGPdD7iF/gicFZEHEF2+v4Z4GMAKaWlEXE98BjZ5Tbnp5TaK151/doPuCF/snITcHVK6aaIuB+4PiLOBZ4DTgfHq2gRMQZ4J/nxk/sHj62RISKuAU4ApkXEKuBvgYsZ/LH0V8AVQCvZPWil96FpmPQxXheRPT3y1vz34r0ppY+TPdXwqxHRBrQDH08pdT7kwvEqsz7G6oR9+N3nWFVAb+OVUvoBPe9fhxo8tiK/ckCSJEmSVOO8BFSSJEmS6oQBUJIkSZLqhAFQkiRJkuqEAVCSJEmS6oQBUJIkSZLqhAFQkqRhFhHvjYgLi65DkqTu/BoISZIkSaoTngGUJGkQImJ+RCyLiO9HxKMRcVVEvCMi7o6IpyLimIj484j457z/FRHxnYj4XUSsiIjTiv4MkqT6ZQCUJGnwDga+DSwCDgE+CLwV+BzwxV76z8rXnwJcXKEaJUnqwQAoSdLgrUwpPZJS6gCWArel7J6KR4D5vfT/eUqpI6X0GLBfBeuUJKkLA6AkSYO3s2S+o2S5A2jaS/8oV1GSJO2NAVCSJEmS6oQBUJIkSZLqhF8DIUmSJEl1wjOAkiRJklQnDICSJEmSVCcMgJIkSZJUJwyAkiRJklQnDICSJEmSVCcMgJIkSZJUJwyAkiRJklQnDICSJEmSVCf+Pz2mMuxcz8IpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sizes.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By filtering out the categories with less than 10 observations, 1566 classes remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classes          1566\n",
       "total_chars    133480\n",
       "Name: 10, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes.loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_train = os.path.join(split_kanji_dir, \"train\")\n",
    "kanji_val = os.path.join(split_kanji_dir, \"val\")\n",
    "kanji_test = os.path.join(split_kanji_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(kanji_train)\n",
    "os.makedirs(kanji_val)\n",
    "os.makedirs(kanji_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_dir_list = directory_counts[directory_counts[\"count\"] >= 10][\"folder\"].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in kanji_dir_list:\n",
    "    paths = image_path_extractor(directory, orig_kuzushiji_kanji_dir, random_seed=SEED)\n",
    "    train, val, test = image_path_list_train_test_split(paths, .7, .1)\n",
    "    \n",
    "    image_path_copier(train, orig_kuzushiji_kanji_dir, kanji_train, directory)\n",
    "    image_path_copier(val, orig_kuzushiji_kanji_dir, kanji_val, directory)\n",
    "    image_path_copier(test, orig_kuzushiji_kanji_dir, kanji_test, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining kanji characters with less than 10 observations might be useful in their aggregate form as a class of their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6944"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes.loc[1][\"total_chars\"]-sizes.loc[10][\"total_chars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4860 files copied to data/kuzushiji49/kanji_transfer/train/other\n",
      "1389 files copied to data/kuzushiji49/kanji_transfer/val/other\n",
      "695 files copied to data/kuzushiji49/kanji_transfer/test/other\n"
     ]
    }
   ],
   "source": [
    "other_dirs = directory_counts[directory_counts[\"count\"] < 10][\"folder\"].to_numpy().tolist()\n",
    "other_paths = image_path_extractor(other_dirs, orig_kuzushiji_kanji_dir, random_seed=SEED)\n",
    "\n",
    "train, val, test = image_path_list_train_test_split(other_paths, .7, .1)\n",
    "\n",
    "#Copies the files to their new subdirectories.\n",
    "image_path_copier(train, orig_kuzushiji_kanji_dir, kanji_train, \"other\")\n",
    "image_path_copier(val, orig_kuzushiji_kanji_dir, kanji_val, \"other\")\n",
    "image_path_copier(test, orig_kuzushiji_kanji_dir, kanji_test, \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train_data = parse_emnist(\"data/emnist/emnist-bymerge-train-images-idx3-ubyte.gz\")\n",
    "emnist_train_labels = parse_emnist(\"data/emnist/emnist-bymerge-train-labels-idx1-ubyte.gz\", offset=8, isimg=False)\n",
    "emnist_test_data = parse_emnist(\"data/emnist/emnist-bymerge-test-images-idx3-ubyte.gz\")\n",
    "emnist_test_labels = parse_emnist(\"data/emnist/emnist-bymerge-test-labels-idx1-ubyte.gz\", offset=8, isimg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train_data = pad_raster_edges(emnist_train_data, 2, 2)\n",
    "emnist_test_data = pad_raster_edges(emnist_test_data, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 unique values with 0 nan values making up 0.0%\n",
      "38304-1 --5.49%\n",
      "36020-7 --5.16%\n",
      "35285-3 --5.06%\n",
      "34618-0 --4.96%\n",
      "34307-2 --4.92%\n",
      "34150-6 --4.89%\n",
      "33924-8 --4.86%\n",
      "33882-9 --4.85%\n",
      "33656-4 --4.82%\n",
      "31280-5 --4.48%\n",
      "27664-24 --3.96%\n",
      "24657-39 --3.53%\n",
      "23509-28 --3.37%\n",
      "20381-21 --2.92%\n",
      "18248-46 --2.61%\n",
      "15388-30 --2.20%\n",
      "14733-18 --2.11%\n",
      "14060-45 --2.01%\n",
      "12963-12 --1.86%\n",
      "11612-22 --1.66%\n",
      "11444-43 --1.64%\n",
      "10748-25 --1.54%\n",
      "10152-38 --1.45%\n",
      "10009-36 --1.43%\n",
      "9766-29 --1.40%\n",
      "9098-15 --1.30%\n",
      "8682-42 --1.24%\n",
      "8237-23 --1.18%\n",
      "7588-31 --1.09%\n",
      "7403-32 --1.06%\n",
      "7092-34 --1.02%\n",
      "6411-10 --0.92%\n",
      "5689-19 --0.82%\n",
      "5598-33 --0.80%\n",
      "5416-35 --0.78%\n",
      "5080-37 --0.73%\n",
      "5047-27 --0.72%\n",
      "4998-20 --0.72%\n",
      "4925-14 --0.71%\n",
      "4606-13 --0.66%\n",
      "3874-11 --0.56%\n",
      "3693-41 --0.53%\n",
      "3097-17 --0.44%\n",
      "2966-44 --0.42%\n",
      "2603-26 --0.37%\n",
      "2535-40 --0.36%\n",
      "2534-16 --0.36%\n"
     ]
    }
   ],
   "source": [
    "full_value_counts(pd.DataFrame(emnist_train_labels), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of observations in this dataset vastly exceed the observations in the kanji set. To prevent these observations from overwhelming the rest of the data, I've downsampled to the lowest value since, 2534 observations is still significant in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing downsample\n",
      "performing downsample\n"
     ]
    }
   ],
   "source": [
    "#My resample function requires each observation to be a vector, so the images are reshaped before resampling\n",
    "emnist_train_data, emnist_train_labels = preprocess_raster_resampling(images_to_1d(emnist_train_data), emnist_train_labels, \"downsample\", random_state=SEED)\n",
    "emnist_test_data, emnist_test_labels = preprocess_raster_resampling(images_to_1d(emnist_test_data), emnist_test_labels, \"downsample\", random_state=SEED)\n",
    "\n",
    "#Creates a validation set\n",
    "emnist_train_X, emnist_val_X, emnist_train_y, emnist_val_y = train_test_split(emnist_train_data, emnist_train_labels, test_size=.2, random_state=SEED)\n",
    "\n",
    "# Restoring the original shape while explicitly adding it's color channel.\n",
    "emnist_train_X = images_to_1d(emnist_train_X, 32, 32, 1, inverse=True)\n",
    "emnist_val_X = images_to_1d(emnist_val_X, 32, 32, 1, inverse=True)\n",
    "emnist_test_data = images_to_1d(emnist_test_data, 32, 32, 1, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "emist_pref = \"emnist_\"\n",
    "export_numpy_array_to_images(emnist_train_X, emnist_train_y, kanji_train, emist_pref)\n",
    "export_numpy_array_to_images(emnist_val_X, emnist_val_y, kanji_val, emist_pref)\n",
    "export_numpy_array_to_images(emnist_test_data, emnist_test_labels, kanji_test, emist_pref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsolete Kana\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it might help to add in those obsolete observations from the hiragana set to build a more robust base for transfer learning. The proprocessing at this stage will be with a lighter touch since it will be heppening after the image sets are exported and reimported with the rest of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "obs_kana_train_data = np.load(\"data/kuzushiji49/train/k49-train-imgs.npz\")['arr_0']\n",
    "obs_kana_train_labels = np.load(\"data/kuzushiji49/train/k49-train-labels.npz\")['arr_0']\n",
    "obs_kana_test_data = np.load(\"data/kuzushiji49/test/k49-test-imgs.npz\")['arr_0']\n",
    "obs_kana_test_labels = np.load(\"data/kuzushiji49/test/k49-test-labels.npz\")['arr_0']\n",
    "\n",
    "# Eliminating the classes that are already used in the main model.\n",
    "obsolete_mask_train = (obs_kana_train_labels == 44) | (obs_kana_train_labels == 45) | (obs_kana_train_labels == 48)\n",
    "obsolete_mask_test = (obs_kana_test_labels == 44) | (obs_kana_test_labels == 45) | (obs_kana_test_labels == 48)\n",
    "obs_kana_train_data, obs_kana_train_labels = obs_kana_train_data[obsolete_mask_train], obs_kana_train_labels[obsolete_mask_train]\n",
    "obs_kana_test_data, obs_kana_test_labels = obs_kana_test_data[obsolete_mask_test], obs_kana_test_labels[obsolete_mask_test]\n",
    "\n",
    "# Match kanji folder structure by replacing the key with the unicode value.\n",
    "to_replace, unicode_sym = 44, \"U+3090\"\n",
    "obs_kana_train_labels, obs_kana_test_labels = np.where(obs_kana_train_labels == to_replace, unicode_sym, obs_kana_train_labels), np.where(obs_kana_test_labels == to_replace, unicode_sym, obs_kana_test_labels)\n",
    "\n",
    "to_replace, unicode_sym = \"45\", \"U+3091\"\n",
    "obs_kana_train_labels, obs_kana_test_labels = np.where(obs_kana_train_labels == to_replace, unicode_sym, obs_kana_train_labels), np.where(obs_kana_test_labels == to_replace, unicode_sym, obs_kana_test_labels)\n",
    "\n",
    "to_replace, unicode_sym = \"48\", \"U+309E\"\n",
    "obs_kana_train_labels, obs_kana_test_labels = np.where(obs_kana_train_labels == to_replace, unicode_sym, obs_kana_train_labels), np.where(obs_kana_test_labels == to_replace, unicode_sym, obs_kana_test_labels)\n",
    "\n",
    "#Adding the padding to make all images 32x32 and allow for edge buffer during transformations.\n",
    "obs_kana_train_data = pad_raster_edges(obs_kana_train_data, 2, 2)\n",
    "obs_kana_test_data = pad_raster_edges(obs_kana_test_data, 2, 2)\n",
    "\n",
    "#Creates a validation set\n",
    "obs_kana_train_X, obs_kana_val_X, obs_kana_train_y, obs_kana_val_y = train_test_split(obs_kana_train_data, obs_kana_train_labels, test_size=.2, random_state=SEED)\n",
    "\n",
    "# Restoring the original shape while explicitly adding it's color channel.\n",
    "obs_kana_train_X = images_to_1d(obs_kana_train_X, 32, 32, 1, inverse=True)\n",
    "obs_kana_val_X = images_to_1d(obs_kana_val_X, 32, 32, 1, inverse=True)\n",
    "obs_kana_test_data = images_to_1d(obs_kana_test_data, 32, 32, 1, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_numpy_array_to_images(obs_kana_train_X, obs_kana_train_y, kanji_train)\n",
    "export_numpy_array_to_images(obs_kana_val_X, obs_kana_val_y, kanji_val)\n",
    "export_numpy_array_to_images(obs_kana_test_data, obs_kana_test_labels, kanji_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprossess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                     rotation_range=4,\n",
    "                                   width_shift_range=0.1, \n",
    "                                   height_shift_range=0.1, \n",
    "                                   shear_range=0.3, \n",
    "                                   zoom_range=0.1, \n",
    "                                   horizontal_flip=False)\n",
    "test_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 196357 images belonging to 1617 classes.\n",
      "Found 52741 images belonging to 1617 classes.\n",
      "Found 34967 images belonging to 1617 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train_generator = train_generator.flow_from_directory(kanji_train, target_size=(64, 64), color_mode='grayscale', seed=SEED, batch_size = 32)\n",
    "X_val_generator = test_generator.flow_from_directory(kanji_val, target_size=(64, 64), color_mode='grayscale', seed=SEED, batch_size = 32)\n",
    "X_test_generator = test_generator.flow_from_directory(kanji_test, target_size=(64, 64), color_mode='grayscale', seed=SEED, batch_size = 14756)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 97614 images belonging to 1567 classes.\n",
      "Found 14756 images belonging to 1567 classes.\n",
      "Accuracy: 0.004467134652201659\n",
      "Balanced Accuracy: 0.0005860734827007006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004467134652201659, 0.0005860734827007006)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummy_train_generator = train_generator.flow_from_directory(kanji_train, target_size=(64, 64), color_mode='grayscale', seed=SEED, batch_size = 1567)\n",
    "X_dummy_test_generator = test_generator.flow_from_directory(kanji_test, target_size=(64, 64), color_mode='grayscale', seed=SEED, batch_size = 1567)\n",
    "\n",
    "dummmy_train, dummy_test = next(X_dummy_train_generator), next(X_dummy_test_generator)\n",
    "\n",
    "dummy_X_train, dummy_y_train = dummmy_train[0], np.argmax(dummmy_train[1], axis=1)\n",
    "dummy_X_test, dummy_y_test = dummy_test[0], np.argmax(dummy_test[1], axis=1)\n",
    "\n",
    "baseline_predictor = DummyClassifier(strategy=\"stratified\", random_state=SEED)\n",
    "baseline_predictor.fit(dummy_X_train, dummy_y_train)\n",
    "y_hat = baseline_predictor.predict(dummy_X_test)\n",
    "image_class_accuracy_scores(dummy_y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "           'balanced',\n",
    "            np.unique(X_train_generator.classes), \n",
    "            X_train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              8193000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1567)              1568567   \n",
      "=================================================================\n",
      "Total params: 10,047,999\n",
      "Trainable params: 10,047,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=(64 ,64,  1)),\n",
    "          Conv2D(32, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(64, (3, 3), activation='relu', padding=\"same\"),\n",
    "          Conv2D(64, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(128, (3, 3), activation='relu', padding=\"same\"),\n",
    "          Conv2D(128, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Flatten(),\n",
    "          Dense(1000, \"relu\"),\n",
    "          Dense(1567, activation='softmax')\n",
    "         ]\n",
    "compile_kwargs = {\"loss\":\"categorical_crossentropy\", \"optimizer\":Adam(learning_rate=0.0001), \"metrics\":['accuracy']}\n",
    "modelk1 = generate_keras_model(Sequential(), layers, compile_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 3050 steps, validate for 876 steps\n",
      "Epoch 1/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 4.0043 - accuracy: 0.3522INFO:tensorflow:Assets written to: model_backups/modelk1-01/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 4.0039 - accuracy: 0.3522 - val_loss: 2.0887 - val_accuracy: 0.6140\n",
      "Epoch 2/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 1.8978 - accuracy: 0.6118INFO:tensorflow:Assets written to: model_backups/modelk1-02/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 1.8978 - accuracy: 0.6119 - val_loss: 1.1777 - val_accuracy: 0.7444\n",
      "Epoch 3/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 1.1891 - accuracy: 0.7251INFO:tensorflow:Assets written to: model_backups/modelk1-03/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 1.1889 - accuracy: 0.7251 - val_loss: 0.8559 - val_accuracy: 0.8012\n",
      "Epoch 4/200\n",
      "3048/3050 [============================>.] - ETA: 0s - loss: 0.8605 - accuracy: 0.7876INFO:tensorflow:Assets written to: model_backups/modelk1-04/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.8604 - accuracy: 0.7876 - val_loss: 0.7286 - val_accuracy: 0.8283\n",
      "Epoch 5/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.6664 - accuracy: 0.8306INFO:tensorflow:Assets written to: model_backups/modelk1-05/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.6664 - accuracy: 0.8306 - val_loss: 0.6376 - val_accuracy: 0.8526\n",
      "Epoch 6/200\n",
      "3048/3050 [============================>.] - ETA: 0s - loss: 0.5308 - accuracy: 0.8596INFO:tensorflow:Assets written to: model_backups/modelk1-06/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.5309 - accuracy: 0.8596 - val_loss: 0.6397 - val_accuracy: 0.8571\n",
      "Epoch 7/200\n",
      "3048/3050 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.8803INFO:tensorflow:Assets written to: model_backups/modelk1-07/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.4389 - accuracy: 0.8803 - val_loss: 0.5674 - val_accuracy: 0.8698\n",
      "Epoch 8/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 0.3668 - accuracy: 0.8989INFO:tensorflow:Assets written to: model_backups/modelk1-08/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.3668 - accuracy: 0.8989 - val_loss: 0.5479 - val_accuracy: 0.8741\n",
      "Epoch 9/200\n",
      "3048/3050 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.9121INFO:tensorflow:Assets written to: model_backups/modelk1-09/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.3075 - accuracy: 0.9121 - val_loss: 0.5513 - val_accuracy: 0.8800\n",
      "Epoch 10/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.2665 - accuracy: 0.9243INFO:tensorflow:Assets written to: model_backups/modelk1-10/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.2666 - accuracy: 0.9243 - val_loss: 0.5647 - val_accuracy: 0.8820\n",
      "Epoch 11/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.2328 - accuracy: 0.9321INFO:tensorflow:Assets written to: model_backups/modelk1-11/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.2328 - accuracy: 0.9321 - val_loss: 0.5448 - val_accuracy: 0.8844\n",
      "Epoch 12/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9403INFO:tensorflow:Assets written to: model_backups/modelk1-12/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.2021 - accuracy: 0.9403 - val_loss: 0.5369 - val_accuracy: 0.8893\n",
      "Epoch 13/200\n",
      "3048/3050 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9465INFO:tensorflow:Assets written to: model_backups/modelk1-13/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.1806 - accuracy: 0.9465 - val_loss: 0.5463 - val_accuracy: 0.8920\n",
      "Epoch 14/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.1636 - accuracy: 0.9509INFO:tensorflow:Assets written to: model_backups/modelk1-14/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.1635 - accuracy: 0.9510 - val_loss: 0.5126 - val_accuracy: 0.8942\n",
      "Epoch 15/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.9566INFO:tensorflow:Assets written to: model_backups/modelk1-15/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.1434 - accuracy: 0.9566 - val_loss: 0.5596 - val_accuracy: 0.8945\n",
      "Epoch 16/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9605INFO:tensorflow:Assets written to: model_backups/modelk1-16/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.1313 - accuracy: 0.9605 - val_loss: 0.5830 - val_accuracy: 0.8948\n",
      "Epoch 17/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9627INFO:tensorflow:Assets written to: model_backups/modelk1-17/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.1215 - accuracy: 0.9627 - val_loss: 0.5645 - val_accuracy: 0.8903\n",
      "Epoch 18/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9652INFO:tensorflow:Assets written to: model_backups/modelk1-18/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.1132 - accuracy: 0.9652 - val_loss: 0.5394 - val_accuracy: 0.8950\n",
      "Epoch 19/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9691INFO:tensorflow:Assets written to: model_backups/modelk1-19/assets\n",
      "3050/3050 [==============================] - 58s 19ms/step - loss: 0.1007 - accuracy: 0.9691 - val_loss: 0.5839 - val_accuracy: 0.8974\n"
     ]
    }
   ],
   "source": [
    "backupsk1 = ModelCheckpoint(\"model_backups/modelk1-{epoch:02d}\", monitor=\"val_loss\")\n",
    "stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, restore_best_weights=True)\n",
    "\n",
    "resultsk1 = modelk1.fit(X_train_generator, epochs=200, steps_per_epoch=3050, validation_data=X_val_generator, validation_steps=876, class_weight=class_weights, callbacks=[backupsk1, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8896719978313906\n",
      "Balanced Accuracy: 0.7921170669058342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8896719978313906, 0.7921170669058342)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(X_test_generator)\n",
    "image_class_evaluation(modelk1, test_batch[0], test_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 196357 images belonging to 1617 classes.\n",
      "Found 52741 images belonging to 1617 classes.\n",
      "Found 34967 images belonging to 1617 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train_generator_sm = train_generator.flow_from_directory(kanji_train, target_size=(32, 32), color_mode='grayscale', seed=SEED, batch_size = 32)\n",
    "X_val_generator_sm = test_generator.flow_from_directory(kanji_val, target_size=(32, 32), color_mode='grayscale', seed=SEED, batch_size = 32)\n",
    "X_test_generator_sm = test_generator.flow_from_directory(kanji_test, target_size=(32, 32), color_mode='grayscale', seed=SEED, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1000)              2049000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1567)              1568567   \n",
      "=================================================================\n",
      "Total params: 3,903,999\n",
      "Trainable params: 3,903,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=(32 ,32,  1)),\n",
    "          Conv2D(32, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(64, (3, 3), activation='relu', padding=\"same\"),\n",
    "          Conv2D(64, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(128, (3, 3), activation='relu', padding=\"same\"),\n",
    "          Conv2D(128, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Flatten(),\n",
    "          Dense(1000, \"relu\"),\n",
    "          Dense(1567, activation='softmax')\n",
    "         ]\n",
    "compile_kwargs = {\"loss\":\"categorical_crossentropy\", \"optimizer\":Adam(learning_rate=0.0001), \"metrics\":['accuracy']}\n",
    "modelk2 = generate_keras_model(Sequential(), layers, compile_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 3050 steps, validate for 876 steps\n",
      "Epoch 1/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 4.4988 - accuracy: 0.2745INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-01/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 4.4977 - accuracy: 0.2746 - val_loss: 2.6937 - val_accuracy: 0.5017\n",
      "Epoch 2/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 2.4055 - accuracy: 0.5219INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-02/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 2.4054 - accuracy: 0.5219 - val_loss: 1.6018 - val_accuracy: 0.6564\n",
      "Epoch 3/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 1.6066 - accuracy: 0.6384INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-03/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 1.6068 - accuracy: 0.6384 - val_loss: 1.2375 - val_accuracy: 0.7182\n",
      "Epoch 4/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 1.2123 - accuracy: 0.7142INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-04/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 1.2122 - accuracy: 0.7142 - val_loss: 0.9789 - val_accuracy: 0.7751\n",
      "Epoch 5/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.9629 - accuracy: 0.7599INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-05/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 0.9626 - accuracy: 0.7600 - val_loss: 0.8578 - val_accuracy: 0.8037\n",
      "Epoch 6/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.7946 - accuracy: 0.7969INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-06/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 0.7943 - accuracy: 0.7969 - val_loss: 0.7851 - val_accuracy: 0.8166\n",
      "Epoch 7/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 0.6671 - accuracy: 0.8242INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-07/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 0.6670 - accuracy: 0.8242 - val_loss: 0.7378 - val_accuracy: 0.8282\n",
      "Epoch 8/200\n",
      "3049/3050 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.8472INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-08/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 0.5704 - accuracy: 0.8472 - val_loss: 0.6963 - val_accuracy: 0.8425\n",
      "Epoch 9/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.4859 - accuracy: 0.8663INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-09/assets\n",
      "3050/3050 [==============================] - 47s 15ms/step - loss: 0.4860 - accuracy: 0.8662 - val_loss: 0.6489 - val_accuracy: 0.8543\n",
      "Epoch 10/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.4227 - accuracy: 0.8811INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-10/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.4227 - accuracy: 0.8811 - val_loss: 0.6373 - val_accuracy: 0.8582\n",
      "Epoch 11/200\n",
      "3048/3050 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8946INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-11/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.3688 - accuracy: 0.8947 - val_loss: 0.6133 - val_accuracy: 0.8649\n",
      "Epoch 12/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.9054INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-12/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.3236 - accuracy: 0.9054 - val_loss: 0.6278 - val_accuracy: 0.8620\n",
      "Epoch 13/200\n",
      "3047/3050 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9144INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-13/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.2871 - accuracy: 0.9145 - val_loss: 0.6010 - val_accuracy: 0.8707\n",
      "Epoch 14/200\n",
      "3048/3050 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9232INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-14/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.2547 - accuracy: 0.9233 - val_loss: 0.6026 - val_accuracy: 0.8734\n",
      "Epoch 15/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9310INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-15/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.2296 - accuracy: 0.9309 - val_loss: 0.6154 - val_accuracy: 0.8701\n",
      "Epoch 16/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9372INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-16/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.2049 - accuracy: 0.9372 - val_loss: 0.6834 - val_accuracy: 0.8633\n",
      "Epoch 17/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.1857 - accuracy: 0.9427INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-17/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.1857 - accuracy: 0.9426 - val_loss: 0.6168 - val_accuracy: 0.8767\n",
      "Epoch 18/200\n",
      "3046/3050 [============================>.] - ETA: 0s - loss: 0.1673 - accuracy: 0.9479INFO:tensorflow:Assets written to: model_backups/kanji/modelk2-18/assets\n",
      "3050/3050 [==============================] - 46s 15ms/step - loss: 0.1673 - accuracy: 0.9479 - val_loss: 0.6313 - val_accuracy: 0.8779\n"
     ]
    }
   ],
   "source": [
    "backupsk2 = ModelCheckpoint(\"model_backups/kanji/modelk2-{epoch:02d}\", monitor=\"val_loss\")\n",
    "stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, restore_best_weights=True)\n",
    "\n",
    "resultsk2 = modelk2.fit(X_train_generator_sm, epochs=200, steps_per_epoch=3050, validation_data=X_val_generator_sm, validation_steps=876, class_weight=class_weights, callbacks=[backupsk2, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84375\n",
      "Balanced Accuracy: 0.8981481481481481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84375, 0.8981481481481481)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(X_test_generator_sm)\n",
    "image_class_evaluation(modelk2, test_batch[0], test_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 196357 images belonging to 1617 classes.\n",
      "Found 34967 images belonging to 1617 classes.\n",
      "Accuracy: 0.012125079770261647\n",
      "Balanced Accuracy: 0.0031651784908406804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.012125079770261647, 0.0031651784908406804)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummy_train_generator = train_generator.flow_from_directory(kanji_train, target_size=(64, 64), color_mode='grayscale', seed=SEED, batch_size = 1567)\n",
    "X_dummy_test_generator = test_generator.flow_from_directory(kanji_test, target_size=(64, 64), color_mode='grayscale', seed=SEED, batch_size = 1567)\n",
    "\n",
    "dummmy_train, dummy_test = next(X_dummy_train_generator), next(X_dummy_test_generator)\n",
    "\n",
    "dummy_X_train, dummy_y_train = dummmy_train[0], np.argmax(dummmy_train[1], axis=1)\n",
    "dummy_X_test, dummy_y_test = dummy_test[0], np.argmax(dummy_test[1], axis=1)\n",
    "\n",
    "baseline_predictor = DummyClassifier(strategy=\"stratified\", random_state=SEED)\n",
    "baseline_predictor.fit(dummy_X_train, dummy_y_train)\n",
    "y_hat = baseline_predictor.predict(dummy_X_test)\n",
    "image_class_accuracy_scores(dummy_y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              2049000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1617)              1618617   \n",
      "=================================================================\n",
      "Total params: 3,954,049\n",
      "Trainable params: 3,954,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=(32 ,32,  1)),\n",
    "          Conv2D(32, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(64, (3, 3), activation='relu', padding=\"same\"),\n",
    "          Conv2D(64, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(128, (3, 3), activation='relu', padding=\"same\"),\n",
    "          Conv2D(128, (3, 3), activation='relu', padding=\"same\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Flatten(),\n",
    "          Dense(1000, \"relu\"),\n",
    "          Dense(1617, activation='softmax')\n",
    "         ]\n",
    "compile_kwargs = {\"loss\":\"categorical_crossentropy\", \"optimizer\":Adam(learning_rate=0.0001), \"metrics\":['accuracy']}\n",
    "modelk3 = generate_keras_model(Sequential(), layers, compile_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 6136 steps, validate for 1648 steps\n",
      "Epoch 1/200\n",
      "6133/6136 [============================>.] - ETA: 0s - loss: 2.8273 - accuracy: 0.4815WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-01/assets\n",
      "6136/6136 [==============================] - 99s 16ms/step - loss: 2.8270 - accuracy: 0.4815 - val_loss: 1.8406 - val_accuracy: 0.6243\n",
      "Epoch 2/200\n",
      "6135/6136 [============================>.] - ETA: 0s - loss: 1.5113 - accuracy: 0.6703INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-02/assets\n",
      "6136/6136 [==============================] - 88s 14ms/step - loss: 1.5112 - accuracy: 0.6703 - val_loss: 1.1645 - val_accuracy: 0.7308\n",
      "Epoch 3/200\n",
      "6132/6136 [============================>.] - ETA: 0s - loss: 1.0453 - accuracy: 0.7438INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-03/assets\n",
      "6136/6136 [==============================] - 89s 14ms/step - loss: 1.0453 - accuracy: 0.7438 - val_loss: 0.8758 - val_accuracy: 0.7838\n",
      "Epoch 4/200\n",
      "6133/6136 [============================>.] - ETA: 0s - loss: 0.8069 - accuracy: 0.7874INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-04/assets\n",
      "6136/6136 [==============================] - 88s 14ms/step - loss: 0.8069 - accuracy: 0.7874 - val_loss: 0.7196 - val_accuracy: 0.8109\n",
      "Epoch 5/200\n",
      "6132/6136 [============================>.] - ETA: 0s - loss: 0.6619 - accuracy: 0.8163INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-05/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.6618 - accuracy: 0.8163 - val_loss: 0.6173 - val_accuracy: 0.8333\n",
      "Epoch 6/200\n",
      "6132/6136 [============================>.] - ETA: 0s - loss: 0.5584 - accuracy: 0.8388INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-06/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.5584 - accuracy: 0.8388 - val_loss: 0.6594 - val_accuracy: 0.8270\n",
      "Epoch 7/200\n",
      "6132/6136 [============================>.] - ETA: 0s - loss: 0.4859 - accuracy: 0.8560INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-07/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.4859 - accuracy: 0.8560 - val_loss: 0.5263 - val_accuracy: 0.8578\n",
      "Epoch 8/200\n",
      "6134/6136 [============================>.] - ETA: 0s - loss: 0.4268 - accuracy: 0.8697INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-08/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.4268 - accuracy: 0.8696 - val_loss: 0.5190 - val_accuracy: 0.8592\n",
      "Epoch 9/200\n",
      "6135/6136 [============================>.] - ETA: 0s - loss: 0.3819 - accuracy: 0.8802INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-09/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.3819 - accuracy: 0.8802 - val_loss: 0.5092 - val_accuracy: 0.8638\n",
      "Epoch 10/200\n",
      "6134/6136 [============================>.] - ETA: 0s - loss: 0.3432 - accuracy: 0.8893INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-10/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.3432 - accuracy: 0.8893 - val_loss: 0.5259 - val_accuracy: 0.8625\n",
      "Epoch 11/200\n",
      "6133/6136 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8961INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-11/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.3130 - accuracy: 0.8961 - val_loss: 0.4600 - val_accuracy: 0.8781\n",
      "Epoch 12/200\n",
      "6132/6136 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9045INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-12/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.2877 - accuracy: 0.9045 - val_loss: 0.4924 - val_accuracy: 0.8760\n",
      "Epoch 13/200\n",
      "6133/6136 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.9083INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-13/assets\n",
      "6136/6136 [==============================] - 89s 15ms/step - loss: 0.2683 - accuracy: 0.9083 - val_loss: 0.4624 - val_accuracy: 0.8819\n",
      "Epoch 14/200\n",
      "6132/6136 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9146INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-14/assets\n",
      "6136/6136 [==============================] - 88s 14ms/step - loss: 0.2469 - accuracy: 0.9146 - val_loss: 0.5154 - val_accuracy: 0.8740\n",
      "Epoch 15/200\n",
      "6135/6136 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9182INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-15/assets\n",
      "6136/6136 [==============================] - 88s 14ms/step - loss: 0.2330 - accuracy: 0.9182 - val_loss: 0.4482 - val_accuracy: 0.8879\n",
      "Epoch 16/200\n",
      "6132/6136 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9225INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-16/assets\n",
      "6136/6136 [==============================] - 88s 14ms/step - loss: 0.2181 - accuracy: 0.9225 - val_loss: 0.4773 - val_accuracy: 0.8842\n",
      "Epoch 17/200\n",
      "6133/6136 [============================>.] - ETA: 0s - loss: 0.2053 - accuracy: 0.9262INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-17/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.2053 - accuracy: 0.9262 - val_loss: 0.4732 - val_accuracy: 0.8836\n",
      "Epoch 18/200\n",
      "6134/6136 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9289INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-18/assets\n",
      "6136/6136 [==============================] - 88s 14ms/step - loss: 0.1964 - accuracy: 0.9289 - val_loss: 0.4696 - val_accuracy: 0.8874\n",
      "Epoch 19/200\n",
      "6135/6136 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9319INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-19/assets\n",
      "6136/6136 [==============================] - 88s 14ms/step - loss: 0.1878 - accuracy: 0.9319 - val_loss: 0.4812 - val_accuracy: 0.8846\n",
      "Epoch 20/200\n",
      "6135/6136 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9335INFO:tensorflow:Assets written to: model_backups/kanji/modelk3-20/assets\n",
      "6136/6136 [==============================] - 87s 14ms/step - loss: 0.1791 - accuracy: 0.9335 - val_loss: 0.4646 - val_accuracy: 0.8909\n"
     ]
    }
   ],
   "source": [
    "backupsk3 = ModelCheckpoint(\"model_backups/kanji/modelk3-{epoch:02d}\", monitor=\"val_loss\")\n",
    "stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, restore_best_weights=True)\n",
    "\n",
    "resultsk3 = modelk3.fit(X_train_generator_sm, epochs=200, steps_per_epoch=6136, validation_data=X_val_generator_sm, validation_steps=1648, class_weight=class_weights, callbacks=[backupsk3, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84375\n",
      "Balanced Accuracy: 0.8392857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84375, 0.8392857142857143)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(X_test_generator_sm)\n",
    "image_class_evaluation(modelk3, test_batch[0], test_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
