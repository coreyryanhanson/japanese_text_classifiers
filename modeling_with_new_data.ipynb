{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import fnmatch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.font_manager as fm\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "from tensorflow.keras import models, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "#User Created functions\n",
    "\n",
    "from cleaning_functions import *\n",
    "from eda_functions import *\n",
    "from modeling_functions import *\n",
    "from setup_functions import *\n",
    "\n",
    "from random_lumberjacks.src.random_lumberjacks.model.model_classes import *\n",
    "from random_lumberjacks.src.random_lumberjacks.visualization.visualization_functions import *\n",
    "\n",
    "#Notebook arguments\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anacuda/work/nyc-mhtn-ds-021720/japanese_text_classifiers/setup_functions.py:23: MatplotlibDeprecationWarning: \n",
      "The createFontList function was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use FontManager.addfont instead.\n",
      "  font_list = fm.createFontList(font_files)\n"
     ]
    }
   ],
   "source": [
    "#Without this block the Japanese font's won't display properly in Matplotlib.Set to your font directory.\n",
    "extend_matplotlib_fonts(\"/usr/share/fonts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device specific gpu deterministic arguments\n",
    "from tensorflow import config as tfconfig\n",
    "physical_devices = tfconfig.list_physical_devices('GPU')\n",
    "tfconfig.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets random seeds to allow for reproducable results.\n",
    "from tensorflow import random as tfrandom\n",
    "SEED=127\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tfrandom.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many classes, I've imported a json that contains useful information about the characters and stored it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiragana_classes = pd.read_json(\"choubenkyo_kivy_app/app/draw_screen/characters/hiragana.json\")\n",
    "\n",
    "# By converting the recorded stroke count to a list of integers, it will be easier to iterate through the data and remove obvious mistakes from the start.\n",
    "hiragana_classes[\"stroke_count\"] = hiragana_classes[\"stroke_count\"].map(lambda x: [int(item[0]) for item in re.finditer(\"\\d+\", x)])\n",
    "\n",
    "# It will be useful to have seperate dataframes to account for dropped classes whether they are the obsolete kana or the compound characters.\n",
    "current_hiragana_classes = hiragana_classes[(hiragana_classes[\"char_id\"]!= \"wi\") & (hiragana_classes[\"char_id\"]!= \"we\")].reset_index(drop=True)\n",
    "reduced_hiragana_classes = current_hiragana_classes.iloc[0:46]\n",
    "compound_hiragana_classes = current_hiragana_classes.iloc[46:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bitmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first goal is to convert all the raw coordinate data collected from the phone app into 28 x 28 greyscale images to match the other datasets. The Pillow library has a function that can render these points to a bitmap image. I've created several functions that ensure that the images are rendered to be centered/filled to the image space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_coordinates_file_to_img(coord_list, resolution=(28, 28), stroke_width=2):\n",
    "    scaled = scale_points_for_pixels(coord_list, resolution, stroke_width)\n",
    "    img = Image.new('L', resolution, color=0)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for coords in scaled:\n",
    "        line_from_array(draw, coords, width=stroke_width)\n",
    "    return img\n",
    "\n",
    "def create_classification_dirs(path_list, labels):\n",
    "    for path in path_list:\n",
    "        for label in labels:\n",
    "            os.makedirs(os.path.join(path, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in order to loop through and QC/arrange the data in an efficient manner a few paths need to be stored as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"data/choubenkyo_data/raw\"\n",
    "rendered_path = \"data/choubenkyo_data/rasterized\"\n",
    "rendered_split_paths = {\"train\": os.path.join(rendered_path, \"train\"),\n",
    "                        \"val\": os.path.join(rendered_path, \"val\"),\n",
    "                        \"test\": os.path.join(rendered_path, \"test\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major change that I made to the raw coordinate data to enhance both the lstm and the bitmap files was to standardize it to a consistent number of points along a b-spline (done using the [NURBS-Python library](https://nurbs-python.readthedocs.io)). Having a consistent shape for all of the parameters is essential for the lstm and if I were to accomplish this through downsampling the points, it would outright ruin the data in many scenarios. The B-spline made the intermediate transitions natural along a curve and while typically use of a b-spline would smooth out corner points in an obtrusive way, the real time capture of the app essentially captures more points during moments where the flow of writing is slowed down (ie. a sharp direction change). With a surplus of control points towards the corners, the smoothing is minimized in places where it wouldn't be desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible results\n",
    "np.random.seed(SEED)\n",
    "\n",
    "max_features=80\n",
    "\n",
    "#Creating the inital train, val, test directories\n",
    "create_classification_dirs(rendered_split_paths.values(), reduced_hiragana_classes[\"char_id\"])\n",
    "\n",
    "# This is the list which will become the lstm input and labels\n",
    "choubenkyo_data = []\n",
    "choubenkyo_raw_labels = []\n",
    "\n",
    "#This list will reference the directory of origin, and the files that are generated/renamed through the train, test, split\n",
    "#to external image file functions.\n",
    "pathmap = []\n",
    "\n",
    "for source in os.listdir(raw_path):\n",
    "    old_dir = os.path.join(raw_path, source)\n",
    "    contents = os.listdir(old_dir)\n",
    "    for label in reduced_hiragana_classes[\"char_id\"]:\n",
    "        matches = fnmatch.filter(contents, f\"{label}*\")\n",
    "        np.random.shuffle(matches)\n",
    "        match_splits = (image_path_list_train_test_split(matches, .7, .1))\n",
    "        new_file_counters = [len(os.path.join(split_dir, label)) for split_dir in rendered_split_paths]\n",
    "        for i,(key, value) in enumerate(rendered_split_paths.items()):\n",
    "            new_dir = os.path.join(value, label)\n",
    "            for match in match_splits[i]:\n",
    "                match_raw_path = os.path.join(old_dir, match)\n",
    "                raw_strokes = load_pickle(match_raw_path)\n",
    "                smoothed_strokes = parse_to_points_list(raw_strokes)\n",
    "                if len(raw_strokes) not in hiragana_classes[hiragana_classes[\"char_id\"]==label][\"stroke_count\"].reset_index(drop=True)[0]:\n",
    "                    pathmap.append([match_raw_path, np.nan])\n",
    "                    continue\n",
    "                new_path = os.path.join(new_dir, f\"{label}{key}{new_file_counters[i]:05}.png\")\n",
    "                \n",
    "                #Exports images for the CNN.\n",
    "                img = render_coordinates_file_to_img(smoothed_strokes, stroke_width=1)\n",
    "                img = ImageOps.expand(img, 2)\n",
    "                img.save(new_path)\n",
    "                \n",
    "                #Saves observations for the lstm\n",
    "                choubenkyo_data.append(strokes_to_array(smoothed_strokes, max_features=max_features))\n",
    "                choubenkyo_raw_labels.append(label)\n",
    "                \n",
    "                #Saves information to the document that preserves the link to the source images.\n",
    "                pathmap.append([match_raw_path, new_path])\n",
    "                \n",
    "                #Tracks changes to image iterator so some searches for existing directories can be avoided.\n",
    "                new_file_counters[i] += 1\n",
    "\n",
    "pathmap = pd.DataFrame(pathmap, columns=[\"orig_file\", \"new_file\"])\n",
    "choubenkyo_data = np.vstack(choubenkyo_data)\n",
    "choubenkyo_raw_labels = pd.Series(choubenkyo_raw_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                     rotation_range=4,\n",
    "                                   width_shift_range=0.1, \n",
    "                                   height_shift_range=0.1, \n",
    "                                   shear_range=0.3, \n",
    "                                   zoom_range=0.1, \n",
    "                                   horizontal_flip=False)\n",
    "test_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 657 images belonging to 46 classes.\n",
      "Found 190 images belonging to 46 classes.\n",
      "Found 118 images belonging to 46 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train_generator = train_generator.flow_from_directory(rendered_split_paths[\"train\"], target_size=(32, 32), color_mode='grayscale', seed=SEED, batch_size = 32)\n",
    "X_val_generator = test_generator.flow_from_directory(rendered_split_paths[\"val\"], target_size=(32, 32), color_mode='grayscale', seed=SEED, batch_size = 32)\n",
    "X_test_generator = test_generator.flow_from_directory(rendered_split_paths[\"test\"], target_size=(32, 32), color_mode='grayscale', seed=SEED, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelkuzushiji = models.load_model('model_backups/kuzushiji/model9-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40)                81960     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                1886      \n",
      "=================================================================\n",
      "Total params: 370,278\n",
      "Trainable params: 305,286\n",
      "Non-trainable params: 64,992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = modelkuzushiji.layers[:-2] + [Dense(40, \"relu\"), Dense(46, activation='softmax')]\n",
    "compile_kwargs = {\"loss\":\"categorical_crossentropy\", \"optimizer\":Adam(learning_rate=0.00005), \"metrics\":['accuracy']}\n",
    "modelc1 = generate_keras_model(Sequential(), layers, compile_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in modelc1.layers[0:3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 20 steps, validate for 5 steps\n",
      "Epoch 1/200\n",
      "13/20 [==================>...........] - ETA: 0s - loss: 5.1505 - accuracy: 0.0224     WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-01/assets\n",
      "20/20 [==============================] - 2s 104ms/step - loss: 4.8982 - accuracy: 0.0240 - val_loss: 4.1028 - val_accuracy: 0.0125\n",
      "Epoch 2/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 3.9305 - accuracy: 0.0409INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-02/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 3.8970 - accuracy: 0.0432 - val_loss: 3.4963 - val_accuracy: 0.0875\n",
      "Epoch 3/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 3.5244 - accuracy: 0.0604INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-03/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 3.4963 - accuracy: 0.0672 - val_loss: 3.1310 - val_accuracy: 0.1312\n",
      "Epoch 4/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 3.1871 - accuracy: 0.1398INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-04/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 3.1230 - accuracy: 0.1584 - val_loss: 2.8222 - val_accuracy: 0.2062\n",
      "Epoch 5/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 2.8673 - accuracy: 0.2305INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-05/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2.8200 - accuracy: 0.2448 - val_loss: 2.5163 - val_accuracy: 0.3000\n",
      "Epoch 6/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 2.6129 - accuracy: 0.3119INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-06/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2.6477 - accuracy: 0.2944 - val_loss: 2.2535 - val_accuracy: 0.3812\n",
      "Epoch 7/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 2.3308 - accuracy: 0.3947INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-07/assets\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2.3302 - accuracy: 0.3920 - val_loss: 1.9817 - val_accuracy: 0.4750\n",
      "Epoch 8/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 2.0650 - accuracy: 0.4688INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-08/assets\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2.0476 - accuracy: 0.4704 - val_loss: 1.7134 - val_accuracy: 0.5437\n",
      "Epoch 9/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 1.8288 - accuracy: 0.5527INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-09/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1.8148 - accuracy: 0.5616 - val_loss: 1.4789 - val_accuracy: 0.6125\n",
      "Epoch 10/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 1.5966 - accuracy: 0.6056INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-10/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1.5812 - accuracy: 0.6192 - val_loss: 1.2538 - val_accuracy: 0.7000\n",
      "Epoch 11/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 1.3762 - accuracy: 0.6667INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-11/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1.3774 - accuracy: 0.6752 - val_loss: 1.0597 - val_accuracy: 0.7437\n",
      "Epoch 12/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 1.2383 - accuracy: 0.6979INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-12/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1.1837 - accuracy: 0.7120 - val_loss: 0.8841 - val_accuracy: 0.7875\n",
      "Epoch 13/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.9986 - accuracy: 0.7808INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-13/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.9963 - accuracy: 0.7824 - val_loss: 0.7365 - val_accuracy: 0.8250\n",
      "Epoch 14/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.8909 - accuracy: 0.7907INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-14/assets\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 0.8652 - accuracy: 0.7952 - val_loss: 0.6133 - val_accuracy: 0.8562\n",
      "Epoch 15/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.7506 - accuracy: 0.8366INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-15/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.7582 - accuracy: 0.8240 - val_loss: 0.5158 - val_accuracy: 0.8750\n",
      "Epoch 16/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6316 - accuracy: 0.8617INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-16/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.6344 - accuracy: 0.8608 - val_loss: 0.4453 - val_accuracy: 0.9000\n",
      "Epoch 17/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.5938 - accuracy: 0.8542INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-17/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.5848 - accuracy: 0.8560 - val_loss: 0.3893 - val_accuracy: 0.9062\n",
      "Epoch 18/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.5165 - accuracy: 0.8833INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-18/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.5116 - accuracy: 0.8800 - val_loss: 0.3282 - val_accuracy: 0.9187\n",
      "Epoch 19/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.4870 - accuracy: 0.8934INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-19/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.4773 - accuracy: 0.8992 - val_loss: 0.2848 - val_accuracy: 0.9312\n",
      "Epoch 20/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.3829 - accuracy: 0.9336INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-20/assets\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 0.4084 - accuracy: 0.9168 - val_loss: 0.2481 - val_accuracy: 0.9312\n",
      "Epoch 21/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.3120 - accuracy: 0.9333INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-21/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.3297 - accuracy: 0.9296 - val_loss: 0.2230 - val_accuracy: 0.9375\n",
      "Epoch 22/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.2711 - accuracy: 0.9398INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-22/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.2606 - accuracy: 0.9424 - val_loss: 0.1999 - val_accuracy: 0.9312\n",
      "Epoch 23/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.2577 - accuracy: 0.9479INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-23/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.2519 - accuracy: 0.9440 - val_loss: 0.1790 - val_accuracy: 0.9375\n",
      "Epoch 24/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.2501 - accuracy: 0.9527INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-24/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.2277 - accuracy: 0.9616 - val_loss: 0.1536 - val_accuracy: 0.9625\n",
      "Epoch 25/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.1831 - accuracy: 0.9646INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-25/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.1844 - accuracy: 0.9648 - val_loss: 0.1358 - val_accuracy: 0.9750\n",
      "Epoch 26/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.1769 - accuracy: 0.9677INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-26/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.1723 - accuracy: 0.9680 - val_loss: 0.1212 - val_accuracy: 0.9750\n",
      "Epoch 27/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1622 - accuracy: 0.9663INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-27/assets\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 0.1631 - accuracy: 0.9680 - val_loss: 0.1140 - val_accuracy: 0.9688\n",
      "Epoch 28/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.1441 - accuracy: 0.9698INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-28/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.1380 - accuracy: 0.9696 - val_loss: 0.1085 - val_accuracy: 0.9625\n",
      "Epoch 29/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1134 - accuracy: 0.9798INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-29/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.1132 - accuracy: 0.9808 - val_loss: 0.1029 - val_accuracy: 0.9688\n",
      "Epoch 30/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.1136 - accuracy: 0.9763INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-30/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.1251 - accuracy: 0.9744 - val_loss: 0.0977 - val_accuracy: 0.9688\n",
      "Epoch 31/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.1107 - accuracy: 0.9792INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-31/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.1038 - accuracy: 0.9808 - val_loss: 0.0964 - val_accuracy: 0.9688\n",
      "Epoch 32/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.0947 - accuracy: 0.9902INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-32/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.1020 - accuracy: 0.9872 - val_loss: 0.0939 - val_accuracy: 0.9688\n",
      "Epoch 33/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.1108 - accuracy: 0.9763INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-33/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.1050 - accuracy: 0.9792 - val_loss: 0.0863 - val_accuracy: 0.9688\n",
      "Epoch 34/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.1037 - accuracy: 0.9759INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-34/assets\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 0.1088 - accuracy: 0.9776 - val_loss: 0.0866 - val_accuracy: 0.9750\n",
      "Epoch 35/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0872 - accuracy: 0.9937INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-35/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0878 - accuracy: 0.9904 - val_loss: 0.0888 - val_accuracy: 0.9688\n",
      "Epoch 36/200\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 0.0721 - accuracy: 0.9911INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-36/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0761 - accuracy: 0.9904 - val_loss: 0.0844 - val_accuracy: 0.9750\n",
      "Epoch 37/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0698 - accuracy: 0.9892INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-37/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0857 - accuracy: 0.9856 - val_loss: 0.0821 - val_accuracy: 0.9750\n",
      "Epoch 38/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0619 - accuracy: 0.9957INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-38/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0663 - accuracy: 0.9920 - val_loss: 0.0855 - val_accuracy: 0.9750\n",
      "Epoch 39/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0569 - accuracy: 0.9966INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-39/assets\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 0.0574 - accuracy: 0.9968 - val_loss: 0.0799 - val_accuracy: 0.9750\n",
      "Epoch 40/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0578 - accuracy: 0.9937INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-40/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0582 - accuracy: 0.9937 - val_loss: 0.0752 - val_accuracy: 0.9750\n",
      "Epoch 41/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.0712 - accuracy: 0.9879INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-41/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0662 - accuracy: 0.9888 - val_loss: 0.0806 - val_accuracy: 0.9750\n",
      "Epoch 42/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.0537 - accuracy: 0.9920INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-42/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0567 - accuracy: 0.9904 - val_loss: 0.0764 - val_accuracy: 0.9750\n",
      "Epoch 43/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0617 - accuracy: 0.9868INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-43/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0598 - accuracy: 0.9872 - val_loss: 0.0693 - val_accuracy: 0.9750\n",
      "Epoch 44/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0573 - accuracy: 0.9849INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-44/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0519 - accuracy: 0.9872 - val_loss: 0.0684 - val_accuracy: 0.9750\n",
      "Epoch 45/200\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0435 - accuracy: 0.9966INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-45/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0418 - accuracy: 0.9968 - val_loss: 0.0682 - val_accuracy: 0.9750\n",
      "Epoch 46/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0390 - accuracy: 0.9978INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-46/assets\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 0.0521 - accuracy: 0.9936 - val_loss: 0.0645 - val_accuracy: 0.9750\n",
      "Epoch 47/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.0365 - accuracy: 0.9960INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-47/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0409 - accuracy: 0.9952 - val_loss: 0.0592 - val_accuracy: 0.9750\n",
      "Epoch 48/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0495 - accuracy: 0.9935INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-48/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0455 - accuracy: 0.9952 - val_loss: 0.0561 - val_accuracy: 0.9750\n",
      "Epoch 49/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0353 - accuracy: 0.9957INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-49/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0406 - accuracy: 0.9936 - val_loss: 0.0559 - val_accuracy: 0.9750\n",
      "Epoch 50/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0285 - accuracy: 0.9979INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-50/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0290 - accuracy: 0.9984 - val_loss: 0.0572 - val_accuracy: 0.9750\n",
      "Epoch 51/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.0399 - accuracy: 0.9920INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-51/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0364 - accuracy: 0.9936 - val_loss: 0.0575 - val_accuracy: 0.9750\n",
      "Epoch 52/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0325 - accuracy: 0.9957INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-52/assets\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 0.0326 - accuracy: 0.9968 - val_loss: 0.0531 - val_accuracy: 0.9750\n",
      "Epoch 53/200\n",
      "16/20 [=======================>......] - ETA: 0s - loss: 0.0385 - accuracy: 0.9940INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-53/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0378 - accuracy: 0.9936 - val_loss: 0.0524 - val_accuracy: 0.9750\n",
      "Epoch 54/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0305 - accuracy: 0.9957INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-54/assets\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.0345 - accuracy: 0.9920 - val_loss: 0.0550 - val_accuracy: 0.9750\n",
      "Epoch 55/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0312 - accuracy: 0.9957INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-55/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0307 - accuracy: 0.9968 - val_loss: 0.0559 - val_accuracy: 0.9750\n",
      "Epoch 56/200\n",
      "15/20 [=====================>........] - ETA: 0s - loss: 0.0236 - accuracy: 0.9937INFO:tensorflow:Assets written to: model_backups/choubenkyo/modelc1-56/assets\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 0.0241 - accuracy: 0.9953 - val_loss: 0.0577 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "backupsc1 = ModelCheckpoint(\"model_backups/choubenkyo/modelc1-{epoch:02d}\", monitor=\"val_loss\")\n",
    "stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, restore_best_weights=True)\n",
    "\n",
    "resultsc1 = modelc1.fit(X_train_generator, epochs=200, steps_per_epoch=20, validation_data=X_val_generator, validation_steps=5, callbacks=[backupsc1, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9375\n",
      "Balanced Accuracy: 0.9285714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9375, 0.9285714285714286)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(X_test_generator)\n",
    "image_class_evaluation(modelc1, test_batch[0], test_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before converting the data, the labels need to be cleaned up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary that will be used to map the character id to an index.\n",
    "reduced_hiragana_label_dict = {key:value for key, value in reduced_hiragana_classes.reset_index()[[\"char_id\",\"index\"]].to_numpy()}\n",
    "\n",
    "#Mapping to the index so that each label is an integer.\n",
    "choubenkyo_labels = choubenkyo_raw_labels.map(reduced_hiragana_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lstm_pipeline(array, labels, pathmap, train_size=None, test_size=None, random_seed=None):\n",
    "    # Reproducible results\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    nobs, sequence_length, nfeatures = array.shape\n",
    "    \n",
    "    #Sets up the parameters for performing the splits on the dataset, depending on vhat values are provided.\n",
    "    if test_size and train_size and train_size != 1-test_size:\n",
    "        split_params = [int(train_size*len(labels)), int((1-test_size)*len(labels))]\n",
    "    elif train_size:\n",
    "        split_params = [int(train_size*len(labels))]\n",
    "    elif test_size:\n",
    "        split_params = [int((1-test_size)*len(labels))]\n",
    "        \n",
    "    #App provides possible values range from -1 to 1. This will standardize the data for machine learning, while\n",
    "    #also preserving information present in the relative size of a drawing.\n",
    "    scaled_array = (array.copy() + 1)/2\n",
    "    \n",
    "    #One hot encodes the labels in order to be fit to the lstm.\n",
    "    labels = to_categorical(labels.copy())\n",
    "    \n",
    "    #In order to keep track of the random shuffling, the pathmap is passed dropping the values that were skipped\n",
    "    #(to maintain the same length and be shuffled the same way).\n",
    "    indices = pathmap.reset_index().dropna()[\"index\"].to_numpy()\n",
    "    \n",
    "    np.random.shuffle(scaled_array)\n",
    "    np.random.shuffle(labels)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    if test_size and train_size and train_size != 1-test_size:\n",
    "        print(\"Performing a train, test, validation split.\")\n",
    "        X_train, X_val, X_test = np.split(scaled_array, split_params)\n",
    "        y_train, y_val, y_test = np.split(labels, split_params)\n",
    "        indices = [np.split(indices, split_params)]\n",
    "        return indices, X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    if test_size or train_size:\n",
    "        print(\"Performing a train, test split.\")\n",
    "        X_train, X_test = np.split(scaled_array, split_params)\n",
    "        y_train, y_test = np.split(labels, split_params)\n",
    "        indices = [np.split(indices, split_params)]\n",
    "        return indices, X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        print(\"Skipping train, test, split\")\n",
    "        return indices, scaled_array, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing a train, test, validation split.\n"
     ]
    }
   ],
   "source": [
    "idx_ref, X_train, X_val, X_test, y_train, y_val, y_test = preprocess_lstm_pipeline(choubenkyo_data, choubenkyo_labels, pathmap, .7, .1, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping train, test, split\n"
     ]
    }
   ],
   "source": [
    "idx_ref, X, y = preprocess_lstm_pipeline(choubenkyo_data, choubenkyo_labels, pathmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 100)               72400     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 40)                4040      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 46)                1886      \n",
      "=================================================================\n",
      "Total params: 78,326\n",
      "Trainable params: 78,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [LSTM(100, input_shape=(120,max_features)),\n",
    "          Dropout(0.1),\n",
    "          Dense(40, activation='relu'),\n",
    "          Dense(46, activation='softmax'),\n",
    "         ]\n",
    "compile_kwargs = {\"loss\":\"categorical_crossentropy\", \"optimizer\":Adam(learning_rate=0.0002), \"metrics\":['accuracy']}\n",
    "modelc2 = generate_keras_model(Sequential(), layers, compile_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "           'balanced',\n",
    "            classes = np.unique(np.argmax(y_train, axis = 1)), \n",
    "            y = np.argmax(y_train, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "char_id          ho\n",
       "character         ほ\n",
       "romanization     ho\n",
       "stroke_count    [4]\n",
       "Name: 29, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_hiragana_classes.iloc[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14410096, 0.4717339 , 0.26027161, 0.44659905, 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [np.array([X_train[3][:,:4].T[0],X_train[3][:,:4].T[2]]), np.array([X_train[3][:,:4].T[1],X_train[3][:,:4].T[3]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 120)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAZUlEQVR4nKXSSw7AIAgE0JH735kulMpvSJOyQp/Mggj8KWUggFIVYNFZwaCCQQWDbiR6sFfDVtdtNR79ZIaENTk+T8k5S/2VJAzRGYMW9FrRaYNXO3y1RdMejxLcWrdtRT/sB8UDZhcZJ2++oIcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7EFF50708210>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_coordinates_file_to_img(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 849 samples, validate on 243 samples\n",
      "Epoch 1/200\n",
      "830/849 [============================>.] - ETA: 0s - loss: 3.8440 - accuracy: 0.0120INFO:tensorflow:Assets written to: model_backups/kanji/modelc2-01/assets\n",
      "849/849 [==============================] - 4s 5ms/sample - loss: 3.8432 - accuracy: 0.0118 - val_loss: 3.8340 - val_accuracy: 0.0123\n",
      "Epoch 2/200\n",
      "840/849 [============================>.] - ETA: 0s - loss: 3.8304 - accuracy: 0.0262INFO:tensorflow:Assets written to: model_backups/kanji/modelc2-02/assets\n",
      "849/849 [==============================] - 3s 4ms/sample - loss: 3.8305 - accuracy: 0.0259 - val_loss: 3.8320 - val_accuracy: 0.0370\n",
      "Epoch 3/200\n",
      "815/849 [===========================>..] - ETA: 0s - loss: 3.8258 - accuracy: 0.0294INFO:tensorflow:Assets written to: model_backups/kanji/modelc2-03/assets\n",
      "849/849 [==============================] - 3s 4ms/sample - loss: 3.8268 - accuracy: 0.0283 - val_loss: 3.8318 - val_accuracy: 0.0370\n",
      "Epoch 4/200\n",
      "845/849 [============================>.] - ETA: 0s - loss: 3.8233 - accuracy: 0.0296INFO:tensorflow:Assets written to: model_backups/kanji/modelc2-04/assets\n",
      "849/849 [==============================] - 3s 4ms/sample - loss: 3.8234 - accuracy: 0.0294 - val_loss: 3.8315 - val_accuracy: 0.0370\n",
      "Epoch 5/200\n",
      "845/849 [============================>.] - ETA: 0s - loss: 3.8225 - accuracy: 0.0178INFO:tensorflow:Assets written to: model_backups/kanji/modelc2-05/assets\n",
      "849/849 [==============================] - 3s 4ms/sample - loss: 3.8221 - accuracy: 0.0177 - val_loss: 3.8324 - val_accuracy: 0.0370\n",
      "Epoch 6/200\n",
      "825/849 [============================>.] - ETA: 0s - loss: 3.8193 - accuracy: 0.0352INFO:tensorflow:Assets written to: model_backups/kanji/modelc2-06/assets\n",
      "849/849 [==============================] - 3s 4ms/sample - loss: 3.8199 - accuracy: 0.0342 - val_loss: 3.8339 - val_accuracy: 0.0370\n",
      "Epoch 7/200\n",
      "830/849 [============================>.] - ETA: 0s - loss: 3.8226 - accuracy: 0.0217INFO:tensorflow:Assets written to: model_backups/kanji/modelc2-07/assets\n",
      "849/849 [==============================] - 3s 4ms/sample - loss: 3.8229 - accuracy: 0.0224 - val_loss: 3.8332 - val_accuracy: 0.0370\n"
     ]
    }
   ],
   "source": [
    "backupsc2 = ModelCheckpoint(\"model_backups/kanji/modelc2-{epoch:02d}\", monitor=\"val_loss\")\n",
    "stopping = EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=5, restore_best_weights=True)\n",
    "\n",
    "resultsc2 = modelc2.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_val, y_val), class_weight=class_weights, callbacks=[backupsc2, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6557377049180327\n",
      "Balanced Accuracy: 0.6544513457556936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6557377049180327, 0.6544513457556936)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_class_evaluation(modelc2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1214,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathmap.reset_index().dropna()[\"index\"].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>orig_file</th>\n",
       "      <th>new_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/choubenkyo_data/raw/batch01/a-cor_0017</td>\n",
       "      <td>data/choubenkyo_data/rasterized/train/a/atrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>data/choubenkyo_data/raw/batch01/a-cor_0013</td>\n",
       "      <td>data/choubenkyo_data/rasterized/train/a/atrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>data/choubenkyo_data/raw/batch01/a-cor_0012</td>\n",
       "      <td>data/choubenkyo_data/rasterized/train/a/atrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>data/choubenkyo_data/raw/batch01/a-cor_0001</td>\n",
       "      <td>data/choubenkyo_data/rasterized/train/a/atrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>data/choubenkyo_data/raw/batch01/a-cor_0016</td>\n",
       "      <td>data/choubenkyo_data/rasterized/train/a/atrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>1216</td>\n",
       "      <td>data/choubenkyo_data/raw/batch02/wo-_0007</td>\n",
       "      <td>data/choubenkyo_data/rasterized/train/wo/wotra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>1217</td>\n",
       "      <td>data/choubenkyo_data/raw/batch02/wo-_0006</td>\n",
       "      <td>data/choubenkyo_data/rasterized/val/wo/woval00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>1218</td>\n",
       "      <td>data/choubenkyo_data/raw/batch02/wo-_0005</td>\n",
       "      <td>data/choubenkyo_data/rasterized/val/wo/woval00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>1219</td>\n",
       "      <td>data/choubenkyo_data/raw/batch02/wo-_0002</td>\n",
       "      <td>data/choubenkyo_data/rasterized/test/wo/wotest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>1220</td>\n",
       "      <td>data/choubenkyo_data/raw/batch02/-n-_0000</td>\n",
       "      <td>data/choubenkyo_data/rasterized/test/-n/-ntest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1221 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                    orig_file  \\\n",
       "0         0  data/choubenkyo_data/raw/batch01/a-cor_0017   \n",
       "1         1  data/choubenkyo_data/raw/batch01/a-cor_0013   \n",
       "2         2  data/choubenkyo_data/raw/batch01/a-cor_0012   \n",
       "3         3  data/choubenkyo_data/raw/batch01/a-cor_0001   \n",
       "4         4  data/choubenkyo_data/raw/batch01/a-cor_0016   \n",
       "...     ...                                          ...   \n",
       "1216   1216    data/choubenkyo_data/raw/batch02/wo-_0007   \n",
       "1217   1217    data/choubenkyo_data/raw/batch02/wo-_0006   \n",
       "1218   1218    data/choubenkyo_data/raw/batch02/wo-_0005   \n",
       "1219   1219    data/choubenkyo_data/raw/batch02/wo-_0002   \n",
       "1220   1220    data/choubenkyo_data/raw/batch02/-n-_0000   \n",
       "\n",
       "                                               new_file  \n",
       "0     data/choubenkyo_data/rasterized/train/a/atrain...  \n",
       "1     data/choubenkyo_data/rasterized/train/a/atrain...  \n",
       "2     data/choubenkyo_data/rasterized/train/a/atrain...  \n",
       "3     data/choubenkyo_data/rasterized/train/a/atrain...  \n",
       "4     data/choubenkyo_data/rasterized/train/a/atrain...  \n",
       "...                                                 ...  \n",
       "1216  data/choubenkyo_data/rasterized/train/wo/wotra...  \n",
       "1217  data/choubenkyo_data/rasterized/val/wo/woval00...  \n",
       "1218  data/choubenkyo_data/rasterized/val/wo/woval00...  \n",
       "1219  data/choubenkyo_data/rasterized/test/wo/wotest...  \n",
       "1220  data/choubenkyo_data/rasterized/test/-n/-ntest...  \n",
       "\n",
       "[1221 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathmap.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1214, 120, 80)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choubenkyo_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'i': 1,\n",
       " 'u': 2,\n",
       " 'e': 3,\n",
       " 'o': 4,\n",
       " 'ka': 5,\n",
       " 'ki': 6,\n",
       " 'ku': 7,\n",
       " 'ke': 8,\n",
       " 'ko': 9,\n",
       " 'sa': 10,\n",
       " 'si': 11,\n",
       " 'su': 12,\n",
       " 'se': 13,\n",
       " 'so': 14,\n",
       " 'ta': 15,\n",
       " 'ti': 16,\n",
       " 'tu': 17,\n",
       " 'te': 18,\n",
       " 'to': 19,\n",
       " 'na': 20,\n",
       " 'ni': 21,\n",
       " 'nu': 22,\n",
       " 'ne': 23,\n",
       " 'no': 24,\n",
       " 'ha': 25,\n",
       " 'hi': 26,\n",
       " 'hu': 27,\n",
       " 'he': 28,\n",
       " 'ho': 29,\n",
       " 'ma': 30,\n",
       " 'mi': 31,\n",
       " 'mu': 32,\n",
       " 'me': 33,\n",
       " 'mo': 34,\n",
       " 'ya': 35,\n",
       " 'yu': 36,\n",
       " 'yo': 37,\n",
       " 'ra': 38,\n",
       " 'ri': 39,\n",
       " 'ru': 40,\n",
       " 're': 41,\n",
       " 'ro': 42,\n",
       " 'wa': 43,\n",
       " 'wo': 44,\n",
       " '-n': 45}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_hiragana_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
